{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch appliqué pour forcer l'utilisation d'IPv4.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import requests.packages.urllib3.util.connection as urllib3_cn\n",
    "\n",
    "# Garder une référence à la fonction originale\n",
    "_original_getaddrinfo = socket.getaddrinfo\n",
    "\n",
    "def patched_getaddrinfo(*args, **kwargs):\n",
    "    \"\"\"Force la résolution d'adresses en IPv4.\"\"\"\n",
    "    responses = _original_getaddrinfo(*args, **kwargs)\n",
    "    # Filtrer pour ne garder que les adresses IPv4 (socket.AF_INET)\n",
    "    return [res for res in responses if res[0] == socket.AF_INET]\n",
    "\n",
    "# Appliquer le patch globalement pour la résolution DNS via socket\n",
    "socket.getaddrinfo = patched_getaddrinfo\n",
    "\n",
    "# Appliquer aussi le patch pour urllib3 (utilisé par certaines bibliothèques http)\n",
    "# Cela peut aider si d'autres appels HTTP indirects sont faits, bien que\n",
    "# le problème principal ici soit gRPC.\n",
    "urllib3_cn.allowed_gai_family = lambda: socket.AF_INET\n",
    "\n",
    "print(\"Patch appliqué pour forcer l'utilisation d'IPv4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clé API chargée avec succès via le fichier .env.\n",
      "Clé API : AIza...\n",
      "Configuration de GenAI réussie.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables depuis le fichier .env dans l'environnement du processus Python actuel\n",
    "load_dotenv()\n",
    "\n",
    "# Maintenant, essayez de lire la variable d'environnement\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"La variable d'environnement GOOGLE_API_KEY n'a pas pu être chargée (vérifiez le fichier .env).\")\n",
    "else:\n",
    "    print(\"Clé API chargée avec succès via le fichier .env.\")\n",
    "    print(f\"Clé API : {GOOGLE_API_KEY[:4]}...\")\n",
    "    # Configurez genai ici, maintenant que la clé est chargée\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        print(\"Configuration de GenAI réussie.\")\n",
    "    except ImportError:\n",
    "        print(\"Erreur: Le module google.generativeai n'est pas installé ou importé.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la configuration de genai : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-2.5-pro-exp-03-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = model.generate_content(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> That's arguably *the* fundamental question humans have pondered across cultures and millennia! There's **no single, universally agreed-upon answer**, and the meaning of life is often considered subjective and deeply personal.\n",
       "> \n",
       "> However, we can explore the different ways people approach this question:\n",
       "> \n",
       "> **1. Philosophical Perspectives:**\n",
       "> \n",
       "> *   **Nihilism:** Argues that life has no inherent, objective meaning, purpose, or intrinsic value.\n",
       "> *   **Existentialism:** Agrees there's no pre-ordained meaning, but emphasizes that individuals are free and responsible for *creating* their own meaning through their choices and actions. (Think Sartre, Camus).\n",
       "> *   **Hedonism:** Suggests the meaning of life is to maximize pleasure and minimize pain.\n",
       "> *   **Stoicism:** Focuses on living virtuously, in accordance with reason and nature, accepting what we cannot control. Meaning comes from inner resilience and ethical action.\n",
       "> *   **Humanism:** Emphasizes human potential, reason, ethics, and flourishing in this life, often focusing on contributing to the greater good of humanity without recourse to the supernatural.\n",
       "> \n",
       "> **2. Religious & Spiritual Perspectives:**\n",
       "> \n",
       "> *   **Theistic Religions (e.g., Christianity, Islam, Judaism):** Often propose that meaning comes from serving a higher power (God), fulfilling divine commandments, achieving salvation or enlightenment, and living in relationship with the creator and fellow beings.\n",
       "> *   **Eastern Religions (e.g., Buddhism, Hinduism):** May focus on achieving enlightenment, breaking the cycle of rebirth (samsara), understanding one's true nature (Atman/Buddha-nature), living in harmony, and reducing suffering through compassion and detachment.\n",
       "> *   **Spirituality (Broader sense):** Finding meaning through connection to something larger than oneself – nature, the universe, a collective consciousness, or an inner sense of purpose.\n",
       "> \n",
       "> **3. Scientific & Biological Perspectives:**\n",
       "> \n",
       "> *   From a purely biological standpoint, the \"purpose\" of life is survival and reproduction – passing on genetic material.\n",
       "> *   Some scientists and thinkers find meaning in the universe's capacity to develop complexity and consciousness – seeing humans as a way the universe experiences itself.\n",
       "> *   However, science primarily describes *how* life works, not necessarily its subjective *meaning* or ultimate *why*.\n",
       "> \n",
       "> **4. Psychological & Personal Perspectives:**\n",
       "> \n",
       "> *   **Finding Purpose:** Many find meaning through having clear goals, pursuing passions, or dedicating themselves to a cause larger than themselves.\n",
       "> *   **Connection & Relationships:** Love, family, friendship, and community provide profound meaning for many people.\n",
       "> *   **Contribution & Service:** Making a positive impact on others or the world.\n",
       "> *   **Growth & Learning:** Continuously developing oneself, gaining knowledge and wisdom.\n",
       "> *   **Experience & Appreciation:** Finding meaning in the simple act of experiencing life, beauty, joy, and even overcoming challenges.\n",
       "> *   **Legacy:** Creating something lasting or influencing future generations.\n",
       "> *   **Logotherapy (Viktor Frankl):** Argues that the primary human drive is not pleasure, but the discovery and pursuit of what we personally find meaningful, which can be found through work, love, and courage in suffering.\n",
       "> \n",
       "> **In Conclusion:**\n",
       "> \n",
       "> Instead of a single answer, the \"meaning of life\" might be:\n",
       "> \n",
       "> *   **Something you discover:** Through introspection, experience, or faith.\n",
       "> *   **Something you create:** Through your choices, actions, and commitments.\n",
       "> *   **A combination of both.**\n",
       "> *   **Found in the journey itself:** The process of living, learning, connecting, and seeking might be where the meaning lies, rather than a final destination or definition.\n",
       "> \n",
       "> Ultimately, the question \"What is the meaning of *my* life?\" is one that each individual must grapple with and answer for themselves. What gives *your* life meaning and purpose?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1: Installation\n",
    "!pip install -q diffusers transformers accelerate ftfy scipy imageio gradio python-dotenv matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 01:06:31.906677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743635191.927049   50997 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743635191.932950   50997 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-03 01:06:31.952947: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tentative d'application du patch IPv4...\n",
      "Fonction getaddrinfo originale : <function getaddrinfo at 0x7f1b28978cc0>\n",
      "Fonction getaddrinfo patchée installée : <function getaddrinfo at 0x7f199d9afd80>\n",
      "Patch urllib3 allowed_gai_family appliqué.\n",
      "Patch IPv4 appliqué avec succès.\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2: Imports\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import imageio # Pour les GIFs\n",
    "\n",
    "# Configuration initiale (GPU, etc.)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "import socket\n",
    "import requests.packages.urllib3.util.connection as urllib3_cn\n",
    "import functools # Bonne pratique pour le wrapping\n",
    "\n",
    "print(\"Tentative d'application du patch IPv4...\")\n",
    "\n",
    "try:\n",
    "    # Vérifier si le patch est déjà appliqué pour éviter les doubles patchs\n",
    "    # (On vérifie si la fonction actuelle a le nom qu'on lui a donné)\n",
    "    if getattr(socket.getaddrinfo, '__qualname__', '') == 'patched_getaddrinfo':\n",
    "         print(\"Le patch IPv4 semble déjà appliqué.\")\n",
    "    else:\n",
    "        # Garder une référence à la fonction originale\n",
    "        _original_getaddrinfo = socket.getaddrinfo\n",
    "        print(f\"Fonction getaddrinfo originale : {_original_getaddrinfo}\")\n",
    "\n",
    "        @functools.wraps(_original_getaddrinfo) # Préserve les métadonnées de la fonction originale\n",
    "        def patched_getaddrinfo(*args, **kwargs):\n",
    "            \"\"\"Force la résolution d'adresses en IPv4 en filtrant les résultats.\"\"\"\n",
    "            # print(f\"Appel de patched_getaddrinfo avec args: {args}, kwargs: {kwargs}\") # Pour débogage\n",
    "            try:\n",
    "                # ----- APPELER L'ORIGINAL SAUVEGARDÉ ICI -----\n",
    "                responses = _original_getaddrinfo(*args, **kwargs)\n",
    "                # ---------------------------------------------\n",
    "\n",
    "                # Filtrer pour ne garder que les adresses IPv4 (socket.AF_INET)\n",
    "                ipv4_responses = [res for res in responses if res[0] == socket.AF_INET]\n",
    "\n",
    "                # print(f\"Réponses originales: {len(responses)}, Réponses IPv4: {len(ipv4_responses)}\") # Pour débogage\n",
    "\n",
    "                # Que faire si aucune adresse IPv4 n'est trouvée ?\n",
    "                # Retourner une liste vide pourrait casser certaines choses.\n",
    "                # Pour l'instant, on retourne seulement les IPv4 si elles existent.\n",
    "                # if not ipv4_responses:\n",
    "                #     print(\"Avertissement: Aucune adresse IPv4 trouvée après filtrage.\")\n",
    "                #     # Option: retourner les réponses originales pour ne pas tout casser ?\n",
    "                #     # return responses\n",
    "\n",
    "                return ipv4_responses\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur à l'intérieur de patched_getaddrinfo : {e}\")\n",
    "                # En cas d'erreur dans le patch, peut-être revenir à l'original ?\n",
    "                # return _original_getaddrinfo(*args, **kwargs)\n",
    "                raise # Ou simplement relancer l'erreur\n",
    "\n",
    "        # Appliquer le patch globalement\n",
    "        socket.getaddrinfo = patched_getaddrinfo\n",
    "        print(f\"Fonction getaddrinfo patchée installée : {socket.getaddrinfo}\")\n",
    "\n",
    "        # Appliquer aussi le patch pour urllib3 (utilisé par certaines bibliothèques http)\n",
    "        # Vérifier si déjà patché pour éviter les erreurs\n",
    "        if not hasattr(urllib3_cn, '_original_allowed_gai_family'):\n",
    "             urllib3_cn._original_allowed_gai_family = getattr(urllib3_cn, 'allowed_gai_family', None) # Sauvegarde si existe\n",
    "             urllib3_cn.allowed_gai_family = lambda: socket.AF_INET\n",
    "             print(\"Patch urllib3 allowed_gai_family appliqué.\")\n",
    "        else:\n",
    "             print(\"urllib3 allowed_gai_family semble déjà patché.\")\n",
    "\n",
    "        print(\"Patch IPv4 appliqué avec succès.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Échec de l'application du patch IPv4 : {e}\")\n",
    "    # Optionnel : essayer de restaurer l'original si le patch a échoué à mi-chemin\n",
    "    if '_original_getaddrinfo' in locals() and hasattr(socket, 'getaddrinfo') and getattr(socket.getaddrinfo, '__qualname__', '') == 'patched_getaddrinfo':\n",
    "         socket.getaddrinfo = _original_getaddrinfo\n",
    "         print(\"Tentative de restauration de getaddrinfo original.\")\n",
    "    if '_original_allowed_gai_family' in getattr(urllib3_cn, '__dict__', {}):\n",
    "         urllib3_cn.allowed_gai_family = urllib3_cn._original_allowed_gai_family\n",
    "         print(\"Tentative de restauration de allowed_gai_family original.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'stable-diffusion-v1-5/stable-diffusion-v1-5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'stable-diffusion-v1-5/stable-diffusion-v1-5' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstable-diffusion-v1-5/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Ou un autre modèle SD\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Charger les composants\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m CLIPTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m text_encoder \u001b[38;5;241m=\u001b[39m CLIPTextModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m vae \u001b[38;5;241m=\u001b[39m AutoencoderKL\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Ajout de l'import\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2197\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'stable-diffusion-v1-5/stable-diffusion-v1-5'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'stable-diffusion-v1-5/stable-diffusion-v1-5' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "# Cellule 3: Chargement du modèle Stable Diffusion\n",
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\" # Ou un autre modèle SD\n",
    "\n",
    "# Charger les composants\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device) # Ajout de l'import\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n",
    "scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\") # Utiliser DDIM comme dans Imagic\n",
    "\n",
    "print(\"Modèle Stable Diffusion chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 4: Préparation de l'entrée\n",
    "input_image_path = \"path/to/your/image.jpg\" # Mettez le chemin de votre image\n",
    "input_image_pil = Image.open(input_image_path).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "target_prompt = \"A photo of a cat wearing a party hat\" # Votre texte cible\n",
    "\n",
    "# Fonction pour prétraiter l'image pour le VAE\n",
    "def preprocess(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h)) # ensure resolution is multiple of 32\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "\n",
    "input_image_tensor = preprocess(input_image_pil).to(device)\n",
    "# Obtenir la latente initiale de l'image d'entrée (utilisée pour la perte de reconstruction)\n",
    "with torch.no_grad():\n",
    "    input_latents = vae.encode(input_image_tensor).latent_dist.sample() * vae.config.scaling_factor # scaling_factor ~0.18215\n",
    "\n",
    "plt.imshow(input_image_pil)\n",
    "plt.title(\"Input Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5: Étape A - Optimisation de l'Embedding\n",
    "\n",
    "# Obtenir e_tgt (non entraînable)\n",
    "text_input_tgt = tokenizer(target_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    e_tgt = text_encoder(text_input_tgt.input_ids.to(device))[0]\n",
    "\n",
    "# Initialiser e_opt comme clone de e_tgt, mais entraînable\n",
    "e_opt = e_tgt.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Configuration de l'optimisation\n",
    "optimizer = optim.Adam([e_opt], lr=1e-3) # Ajuster lr\n",
    "num_optimization_steps = 100 # Comme dans Imagic (~100), ajuster si besoin\n",
    "unet.eval() # Garder UNet gelé pendant cette étape\n",
    "text_encoder.eval() # Garder text_encoder gelé\n",
    "vae.eval() # Garder VAE gelé\n",
    "\n",
    "print(\"Début de l'optimisation de l'embedding...\")\n",
    "pbar = tqdm(range(num_optimization_steps))\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Échantillonner un timestep aléatoire\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "\n",
    "    # Bruiter l'image latente d'entrée\n",
    "    noise = torch.randn_like(input_latents)\n",
    "    latents_noisy = scheduler.add_noise(input_latents, noise, t)\n",
    "\n",
    "    # Prédire le bruit avec e_opt\n",
    "    noise_pred = unet(latents_noisy, t, encoder_hidden_states=e_opt).sample\n",
    "\n",
    "    # Calculer la perte de reconstruction\n",
    "    loss = nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Optimisation de l'embedding terminée.\")\n",
    "e_opt_optimized = e_opt.clone().detach() # Sauvegarder l'embedding optimisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 6: Étape B - Fine-tuning UNet (Simplifié - Envisager LoRA en pratique)\n",
    "\n",
    "# Rendre UNet entraînable, geler le reste\n",
    "unet.train()\n",
    "text_encoder.eval()\n",
    "vae.eval()\n",
    "# e_opt_optimized n'a pas besoin de grad ici\n",
    "\n",
    "optimizer_unet = optim.Adam(unet.parameters(), lr=5e-6) # Très petit lr pour fine-tuning\n",
    "num_finetune_steps = 200 # Ajuster (~1500 dans Imagic, mais long/coûteux)\n",
    "\n",
    "print(\"Début du fine-tuning de UNet...\")\n",
    "pbar = tqdm(range(num_finetune_steps))\n",
    "for step in pbar:\n",
    "    optimizer_unet.zero_grad()\n",
    "\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "    noise = torch.randn_like(input_latents)\n",
    "    latents_noisy = scheduler.add_noise(input_latents, noise, t)\n",
    "\n",
    "    # Prédire le bruit avec le UNet actuel et e_opt_optimized\n",
    "    noise_pred = unet(latents_noisy, t, encoder_hidden_states=e_opt_optimized).sample\n",
    "\n",
    "    loss = nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_unet.step()\n",
    "\n",
    "    pbar.set_description(f\"UNet Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Fine-tuning UNet terminé.\")\n",
    "# Sauvegarder les poids fine-tunés si nécessaire (ou les poids LoRA)\n",
    "# unet.save_pretrained(\"./unet_finetuned\")\n",
    "unet.eval() # Remettre en mode évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7: Étape C - Interpolation et Génération\n",
    "\n",
    "eta = 0.7 # Facteur d'interpolation (entre 0 et 1) - à ajuster\n",
    "guidance_scale = 7.5 # Force de la guidance (CFG)\n",
    "num_inference_steps = 50 # Nombre d'étapes de sampling\n",
    "\n",
    "# Calculer l'embedding interpolé\n",
    "e_bar = eta * e_tgt + (1 - eta) * e_opt_optimized\n",
    "\n",
    "# Préparer pour CFG (Classifier-Free Guidance)\n",
    "uncond_input = tokenizer([\"\"] * 1, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "text_embeddings = torch.cat([uncond_embeddings, e_bar])\n",
    "\n",
    "# Initialiser la latente de départ (bruit pur)\n",
    "latents = torch.randn((1, unet.config.in_channels, 512 // 8, 512 // 8), device=device) # Taille pour SD 1.5\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "# Boucle de sampling DDIM\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "print(\"Début de la génération de l'image éditée...\")\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # Expansion des latentes pour CFG\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "    # Prédire le bruit\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # CFG\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # Calculer l'étape précédente de la latente\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "print(\"Génération terminée.\")\n",
    "\n",
    "# Décoder l'image finale\n",
    "latents = 1 / vae.config.scaling_factor * latents\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "edited_image_pil = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "# Afficher l'image éditée\n",
    "plt.imshow(edited_image_pil)\n",
    "plt.title(f\"Edited Image (eta={eta})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 8 (Concept): Contrôle par Attention Croisée\n",
    "\n",
    "# --- Cette partie est complexe et nécessite une compréhension profonde ---\n",
    "# --- de Prompt-to-Prompt et de l'architecture de CrossAttention de diffusers ---\n",
    "\n",
    "# 1. Créer des classes AttentionProcessor personnalisées\n",
    "class SaveAttentionProcessor:\n",
    "    def __init__(self):\n",
    "        self.attention_maps = {} # Dictionnaire pour stocker les cartes\n",
    "\n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        # ... logique pour calculer l'attention normalement ...\n",
    "        # SAUVEGARDER la carte d'attention calculée (attn.attention_probs)\n",
    "        # dans self.attention_maps avec une clé unique (ex: nom de couche, timestep)\n",
    "        # ... retourner la sortie de l'attention ...\n",
    "        return # output\n",
    "\n",
    "class InjectAttentionProcessor:\n",
    "    def __init__(self, saved_maps_source):\n",
    "        self.saved_maps = saved_maps_source # Référence aux cartes sauvegardées\n",
    "\n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        # ... début du calcul de l'attention avec le prompt cible ...\n",
    "        # RÉCUPÉRER la carte d'attention source correspondante depuis self.saved_maps\n",
    "        # MODIFIER la carte d'attention cible en utilisant la carte source\n",
    "        # (ex: remplacement partiel basé sur les tokens, etc.)\n",
    "        # ... finir le calcul de l'attention avec la carte modifiée ...\n",
    "        # ... retourner la sortie de l'attention ...\n",
    "        return # output\n",
    "\n",
    "# 2. Modifier la boucle de sampling (Étape C)\n",
    "\n",
    "# -- Au début de la boucle de sampling --\n",
    "# att_saver = SaveAttentionProcessor()\n",
    "# unet.set_attn_processor(att_saver) # Attacher le saver\n",
    "\n",
    "# latents_source = latents.clone() # Bruit initial pour la passe source\n",
    "\n",
    "# -- Boucle sur les timesteps t --\n",
    "\n",
    "    # --- Passe Source (pour sauvegarder l'attention) ---\n",
    "    # latent_model_input_source = scheduler.scale_model_input(latents_source, t)\n",
    "    # with torch.no_grad():\n",
    "    #     # Utiliser e_opt_optimized pour la passe source\n",
    "    #     _ = unet(latent_model_input_source, t, encoder_hidden_states=e_opt_optimized).sample\n",
    "    # # Ici, att_saver.attention_maps devrait être rempli pour ce timestep\n",
    "\n",
    "    # --- Passe Cible (avec injection d'attention) ---\n",
    "    # att_injector = InjectAttentionProcessor(att_saver.attention_maps)\n",
    "    # unet.set_attn_processor(att_injector) # Attacher l'injector\n",
    "\n",
    "    # latent_model_input_target = torch.cat([latents] * 2) # Pour CFG\n",
    "    # latent_model_input_target = scheduler.scale_model_input(latent_model_input_target, t)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     # Utiliser e_tgt (ou e_bar) pour la passe cible\n",
    "    #     noise_pred = unet(latent_model_input_target, t, encoder_hidden_states=text_embeddings).sample # text_embeddings utilise e_tgt/e_bar\n",
    "\n",
    "    # --- Fin de la passe cible ---\n",
    "    # unet.set_attn_processor(None) # Détacher les processors (ou remettre les originaux)\n",
    "\n",
    "    # ... reste de la logique CFG et scheduler.step(noise_pred, t, latents) ...\n",
    "\n",
    "# --- Fin de la boucle ---\n",
    "\n",
    "# 3. Décoder l'image finale comme avant\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
