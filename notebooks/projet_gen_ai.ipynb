{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch appliqué pour forcer l'utilisation d'IPv4.\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import requests.packages.urllib3.util.connection as urllib3_cn\n",
    "\n",
    "# Garder une référence à la fonction originale\n",
    "_original_getaddrinfo = socket.getaddrinfo\n",
    "\n",
    "def patched_getaddrinfo(*args, **kwargs):\n",
    "    \"\"\"Force la résolution d'adresses en IPv4.\"\"\"\n",
    "    responses = _original_getaddrinfo(*args, **kwargs)\n",
    "    # Filtrer pour ne garder que les adresses IPv4 (socket.AF_INET)\n",
    "    return [res for res in responses if res[0] == socket.AF_INET]\n",
    "\n",
    "# Appliquer le patch globalement pour la résolution DNS via socket\n",
    "socket.getaddrinfo = patched_getaddrinfo\n",
    "\n",
    "# Appliquer aussi le patch pour urllib3 (utilisé par certaines bibliothèques http)\n",
    "# Cela peut aider si d'autres appels HTTP indirects sont faits, bien que\n",
    "# le problème principal ici soit gRPC.\n",
    "urllib3_cn.allowed_gai_family = lambda: socket.AF_INET\n",
    "\n",
    "print(\"Patch appliqué pour forcer l'utilisation d'IPv4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "La variable d'environnement GOOGLE_API_KEY n'a pas pu être chargée (vérifiez le fichier .env).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m GOOGLE_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m GOOGLE_API_KEY:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa variable d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menvironnement GOOGLE_API_KEY n\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma pas pu être chargée (vérifiez le fichier .env).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClé API chargée avec succès via le fichier .env.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: La variable d'environnement GOOGLE_API_KEY n'a pas pu être chargée (vérifiez le fichier .env)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables depuis le fichier .env dans l'environnement du processus Python actuel\n",
    "load_dotenv()\n",
    "\n",
    "# Maintenant, essayez de lire la variable d'environnement\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"La variable d'environnement GOOGLE_API_KEY n'a pas pu être chargée (vérifiez le fichier .env).\")\n",
    "else:\n",
    "    print(\"Clé API chargée avec succès via le fichier .env.\")\n",
    "    print(f\"Clé API : {GOOGLE_API_KEY[:4]}...\")\n",
    "    # Configurez genai ici, maintenant que la clé est chargée\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        print(\"Configuration de GenAI réussie.\")\n",
    "    except ImportError:\n",
    "        print(\"Erreur: Le module google.generativeai n'est pas installé ou importé.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la configuration de genai : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-2.5-pro-exp-03-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = model.generate_content(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> That's arguably *the* fundamental question humans have pondered across cultures and millennia! There's **no single, universally agreed-upon answer**, and the meaning of life is often considered subjective and deeply personal.\n",
       "> \n",
       "> However, we can explore the different ways people approach this question:\n",
       "> \n",
       "> **1. Philosophical Perspectives:**\n",
       "> \n",
       "> *   **Nihilism:** Argues that life has no inherent, objective meaning, purpose, or intrinsic value.\n",
       "> *   **Existentialism:** Agrees there's no pre-ordained meaning, but emphasizes that individuals are free and responsible for *creating* their own meaning through their choices and actions. (Think Sartre, Camus).\n",
       "> *   **Hedonism:** Suggests the meaning of life is to maximize pleasure and minimize pain.\n",
       "> *   **Stoicism:** Focuses on living virtuously, in accordance with reason and nature, accepting what we cannot control. Meaning comes from inner resilience and ethical action.\n",
       "> *   **Humanism:** Emphasizes human potential, reason, ethics, and flourishing in this life, often focusing on contributing to the greater good of humanity without recourse to the supernatural.\n",
       "> \n",
       "> **2. Religious & Spiritual Perspectives:**\n",
       "> \n",
       "> *   **Theistic Religions (e.g., Christianity, Islam, Judaism):** Often propose that meaning comes from serving a higher power (God), fulfilling divine commandments, achieving salvation or enlightenment, and living in relationship with the creator and fellow beings.\n",
       "> *   **Eastern Religions (e.g., Buddhism, Hinduism):** May focus on achieving enlightenment, breaking the cycle of rebirth (samsara), understanding one's true nature (Atman/Buddha-nature), living in harmony, and reducing suffering through compassion and detachment.\n",
       "> *   **Spirituality (Broader sense):** Finding meaning through connection to something larger than oneself – nature, the universe, a collective consciousness, or an inner sense of purpose.\n",
       "> \n",
       "> **3. Scientific & Biological Perspectives:**\n",
       "> \n",
       "> *   From a purely biological standpoint, the \"purpose\" of life is survival and reproduction – passing on genetic material.\n",
       "> *   Some scientists and thinkers find meaning in the universe's capacity to develop complexity and consciousness – seeing humans as a way the universe experiences itself.\n",
       "> *   However, science primarily describes *how* life works, not necessarily its subjective *meaning* or ultimate *why*.\n",
       "> \n",
       "> **4. Psychological & Personal Perspectives:**\n",
       "> \n",
       "> *   **Finding Purpose:** Many find meaning through having clear goals, pursuing passions, or dedicating themselves to a cause larger than themselves.\n",
       "> *   **Connection & Relationships:** Love, family, friendship, and community provide profound meaning for many people.\n",
       "> *   **Contribution & Service:** Making a positive impact on others or the world.\n",
       "> *   **Growth & Learning:** Continuously developing oneself, gaining knowledge and wisdom.\n",
       "> *   **Experience & Appreciation:** Finding meaning in the simple act of experiencing life, beauty, joy, and even overcoming challenges.\n",
       "> *   **Legacy:** Creating something lasting or influencing future generations.\n",
       "> *   **Logotherapy (Viktor Frankl):** Argues that the primary human drive is not pleasure, but the discovery and pursuit of what we personally find meaningful, which can be found through work, love, and courage in suffering.\n",
       "> \n",
       "> **In Conclusion:**\n",
       "> \n",
       "> Instead of a single answer, the \"meaning of life\" might be:\n",
       "> \n",
       "> *   **Something you discover:** Through introspection, experience, or faith.\n",
       "> *   **Something you create:** Through your choices, actions, and commitments.\n",
       "> *   **A combination of both.**\n",
       "> *   **Found in the journey itself:** The process of living, learning, connecting, and seeking might be where the meaning lies, rather than a final destination or definition.\n",
       "> \n",
       "> Ultimately, the question \"What is the meaning of *my* life?\" is one that each individual must grapple with and answer for themselves. What gives *your* life meaning and purpose?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 1: Installation\n",
    "!pip install -q diffusers transformers accelerate ftfy scipy imageio gradio python-dotenv matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tentative d'application du patch IPv4...\n",
      "Le patch IPv4 semble déjà appliqué.\n"
     ]
    }
   ],
   "source": [
    "# Cellule 2: Imports\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import imageio # Pour les GIFs\n",
    "\n",
    "# Configuration initiale (GPU, etc.)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "import socket\n",
    "import requests.packages.urllib3.util.connection as urllib3_cn\n",
    "import functools # Bonne pratique pour le wrapping\n",
    "\n",
    "print(\"Tentative d'application du patch IPv4...\")\n",
    "\n",
    "try:\n",
    "    # Vérifier si le patch est déjà appliqué pour éviter les doubles patchs\n",
    "    # (On vérifie si la fonction actuelle a le nom qu'on lui a donné)\n",
    "    if getattr(socket.getaddrinfo, '__qualname__', '') == 'patched_getaddrinfo':\n",
    "         print(\"Le patch IPv4 semble déjà appliqué.\")\n",
    "    else:\n",
    "        # Garder une référence à la fonction originale\n",
    "        _original_getaddrinfo = socket.getaddrinfo\n",
    "        print(f\"Fonction getaddrinfo originale : {_original_getaddrinfo}\")\n",
    "\n",
    "        @functools.wraps(_original_getaddrinfo) # Préserve les métadonnées de la fonction originale\n",
    "        def patched_getaddrinfo(*args, **kwargs):\n",
    "            \"\"\"Force la résolution d'adresses en IPv4 en filtrant les résultats.\"\"\"\n",
    "            # print(f\"Appel de patched_getaddrinfo avec args: {args}, kwargs: {kwargs}\") # Pour débogage\n",
    "            try:\n",
    "                # ----- APPELER L'ORIGINAL SAUVEGARDÉ ICI -----\n",
    "                responses = _original_getaddrinfo(*args, **kwargs)\n",
    "                # ---------------------------------------------\n",
    "\n",
    "                # Filtrer pour ne garder que les adresses IPv4 (socket.AF_INET)\n",
    "                ipv4_responses = [res for res in responses if res[0] == socket.AF_INET]\n",
    "\n",
    "                # print(f\"Réponses originales: {len(responses)}, Réponses IPv4: {len(ipv4_responses)}\") # Pour débogage\n",
    "\n",
    "                # Que faire si aucune adresse IPv4 n'est trouvée ?\n",
    "                # Retourner une liste vide pourrait casser certaines choses.\n",
    "                # Pour l'instant, on retourne seulement les IPv4 si elles existent.\n",
    "                # if not ipv4_responses:\n",
    "                #     print(\"Avertissement: Aucune adresse IPv4 trouvée après filtrage.\")\n",
    "                #     # Option: retourner les réponses originales pour ne pas tout casser ?\n",
    "                #     # return responses\n",
    "\n",
    "                return ipv4_responses\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur à l'intérieur de patched_getaddrinfo : {e}\")\n",
    "                # En cas d'erreur dans le patch, peut-être revenir à l'original ?\n",
    "                # return _original_getaddrinfo(*args, **kwargs)\n",
    "                raise # Ou simplement relancer l'erreur\n",
    "\n",
    "        # Appliquer le patch globalement\n",
    "        socket.getaddrinfo = patched_getaddrinfo\n",
    "        print(f\"Fonction getaddrinfo patchée installée : {socket.getaddrinfo}\")\n",
    "\n",
    "        # Appliquer aussi le patch pour urllib3 (utilisé par certaines bibliothèques http)\n",
    "        # Vérifier si déjà patché pour éviter les erreurs\n",
    "        if not hasattr(urllib3_cn, '_original_allowed_gai_family'):\n",
    "             urllib3_cn._original_allowed_gai_family = getattr(urllib3_cn, 'allowed_gai_family', None) # Sauvegarde si existe\n",
    "             urllib3_cn.allowed_gai_family = lambda: socket.AF_INET\n",
    "             print(\"Patch urllib3 allowed_gai_family appliqué.\")\n",
    "        else:\n",
    "             print(\"urllib3 allowed_gai_family semble déjà patché.\")\n",
    "\n",
    "        print(\"Patch IPv4 appliqué avec succès.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Échec de l'application du patch IPv4 : {e}\")\n",
    "    # Optionnel : essayer de restaurer l'original si le patch a échoué à mi-chemin\n",
    "    if '_original_getaddrinfo' in locals() and hasattr(socket, 'getaddrinfo') and getattr(socket.getaddrinfo, '__qualname__', '') == 'patched_getaddrinfo':\n",
    "         socket.getaddrinfo = _original_getaddrinfo\n",
    "         print(\"Tentative de restauration de getaddrinfo original.\")\n",
    "    if '_original_allowed_gai_family' in getattr(urllib3_cn, '__dict__', {}):\n",
    "         urllib3_cn.allowed_gai_family = urllib3_cn._original_allowed_gai_family\n",
    "         print(\"Tentative de restauration de allowed_gai_family original.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative de chargement avec ID: stabilityai/stable-diffusion-3.5-medium\n",
      "ERREUR lors du chargement de stabilityai/stable-diffusion-3.5-medium: Can't load tokenizer for 'stabilityai/stable-diffusion-3.5-medium'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'stabilityai/stable-diffusion-3.5-medium' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer.\n",
      "Vérifiez la connectivité réseau, le nom du modèle et les problèmes de cache.\n"
     ]
    }
   ],
   "source": [
    "# AU LIEU DE: model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# ESSAYEZ CECI:\n",
    "model_id = \"stabilityai/stable-diffusion-3.5-medium\" # Maintenu par Stability AI (plus fiable)\n",
    "# OU potentiellement juste:\n",
    "# model_id = \"stable-diffusion-v1-5\" # Parfois Hugging Face a des alias directs\n",
    "\n",
    "# --- Le reste de votre code de chargement ---\n",
    "try:\n",
    "    print(f\"Tentative de chargement avec ID: {model_id}\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n",
    "    # Assurez-vous d'avoir importé AutoencoderKL\n",
    "    from diffusers import AutoencoderKL\n",
    "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    print(f\"Modèle {model_id} chargé avec succès.\")\n",
    "\n",
    "except OSError as e:\n",
    "    print(f\"ERREUR lors du chargement de {model_id}: {e}\")\n",
    "    print(\"Vérifiez la connectivité réseau, le nom du modèle et les problèmes de cache.\")\n",
    "    # Ici, vous pourriez essayer un autre ID ou passer au chargement local si nécessaire.\n",
    "except Exception as e:\n",
    "     print(f\"Une erreur inattendue est survenue : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 4: Préparation de l'entrée\n",
    "input_image_path = \"path/to/your/image.jpg\" # Mettez le chemin de votre image\n",
    "input_image_pil = Image.open(input_image_path).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "target_prompt = \"A photo of a cat wearing a party hat\" # Votre texte cible\n",
    "\n",
    "# Fonction pour prétraiter l'image pour le VAE\n",
    "def preprocess(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h)) # ensure resolution is multiple of 32\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "\n",
    "input_image_tensor = preprocess(input_image_pil).to(device)\n",
    "# Obtenir la latente initiale de l'image d'entrée (utilisée pour la perte de reconstruction)\n",
    "with torch.no_grad():\n",
    "    input_latents = vae.encode(input_image_tensor).latent_dist.sample() * vae.config.scaling_factor # scaling_factor ~0.18215\n",
    "\n",
    "plt.imshow(input_image_pil)\n",
    "plt.title(\"Input Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 5: Étape A - Optimisation de l'Embedding\n",
    "\n",
    "# Obtenir e_tgt (non entraînable)\n",
    "text_input_tgt = tokenizer(target_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    e_tgt = text_encoder(text_input_tgt.input_ids.to(device))[0]\n",
    "\n",
    "# Initialiser e_opt comme clone de e_tgt, mais entraînable\n",
    "e_opt = e_tgt.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Configuration de l'optimisation\n",
    "optimizer = optim.Adam([e_opt], lr=1e-3) # Ajuster lr\n",
    "num_optimization_steps = 100 # Comme dans Imagic (~100), ajuster si besoin\n",
    "unet.eval() # Garder UNet gelé pendant cette étape\n",
    "text_encoder.eval() # Garder text_encoder gelé\n",
    "vae.eval() # Garder VAE gelé\n",
    "\n",
    "print(\"Début de l'optimisation de l'embedding...\")\n",
    "pbar = tqdm(range(num_optimization_steps))\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Échantillonner un timestep aléatoire\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "\n",
    "    # Bruiter l'image latente d'entrée\n",
    "    noise = torch.randn_like(input_latents)\n",
    "    latents_noisy = scheduler.add_noise(input_latents, noise, t)\n",
    "\n",
    "    # Prédire le bruit avec e_opt\n",
    "    noise_pred = unet(latents_noisy, t, encoder_hidden_states=e_opt).sample\n",
    "\n",
    "    # Calculer la perte de reconstruction\n",
    "    loss = nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Optimisation de l'embedding terminée.\")\n",
    "e_opt_optimized = e_opt.clone().detach() # Sauvegarder l'embedding optimisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 6: Étape B - Fine-tuning UNet (Simplifié - Envisager LoRA en pratique)\n",
    "\n",
    "# Rendre UNet entraînable, geler le reste\n",
    "unet.train()\n",
    "text_encoder.eval()\n",
    "vae.eval()\n",
    "# e_opt_optimized n'a pas besoin de grad ici\n",
    "\n",
    "optimizer_unet = optim.Adam(unet.parameters(), lr=5e-6) # Très petit lr pour fine-tuning\n",
    "num_finetune_steps = 200 # Ajuster (~1500 dans Imagic, mais long/coûteux)\n",
    "\n",
    "print(\"Début du fine-tuning de UNet...\")\n",
    "pbar = tqdm(range(num_finetune_steps))\n",
    "for step in pbar:\n",
    "    optimizer_unet.zero_grad()\n",
    "\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "    noise = torch.randn_like(input_latents)\n",
    "    latents_noisy = scheduler.add_noise(input_latents, noise, t)\n",
    "\n",
    "    # Prédire le bruit avec le UNet actuel et e_opt_optimized\n",
    "    noise_pred = unet(latents_noisy, t, encoder_hidden_states=e_opt_optimized).sample\n",
    "\n",
    "    loss = nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_unet.step()\n",
    "\n",
    "    pbar.set_description(f\"UNet Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Fine-tuning UNet terminé.\")\n",
    "# Sauvegarder les poids fine-tunés si nécessaire (ou les poids LoRA)\n",
    "# unet.save_pretrained(\"./unet_finetuned\")\n",
    "unet.eval() # Remettre en mode évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 7: Étape C - Interpolation et Génération\n",
    "\n",
    "eta = 0.7 # Facteur d'interpolation (entre 0 et 1) - à ajuster\n",
    "guidance_scale = 7.5 # Force de la guidance (CFG)\n",
    "num_inference_steps = 50 # Nombre d'étapes de sampling\n",
    "\n",
    "# Calculer l'embedding interpolé\n",
    "e_bar = eta * e_tgt + (1 - eta) * e_opt_optimized\n",
    "\n",
    "# Préparer pour CFG (Classifier-Free Guidance)\n",
    "uncond_input = tokenizer([\"\"] * 1, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "text_embeddings = torch.cat([uncond_embeddings, e_bar])\n",
    "\n",
    "# Initialiser la latente de départ (bruit pur)\n",
    "latents = torch.randn((1, unet.config.in_channels, 512 // 8, 512 // 8), device=device) # Taille pour SD 1.5\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "# Boucle de sampling DDIM\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "print(\"Début de la génération de l'image éditée...\")\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # Expansion des latentes pour CFG\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "    # Prédire le bruit\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # CFG\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # Calculer l'étape précédente de la latente\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "print(\"Génération terminée.\")\n",
    "\n",
    "# Décoder l'image finale\n",
    "latents = 1 / vae.config.scaling_factor * latents\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "edited_image_pil = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "# Afficher l'image éditée\n",
    "plt.imshow(edited_image_pil)\n",
    "plt.title(f\"Edited Image (eta={eta})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule 8 (Concept): Contrôle par Attention Croisée\n",
    "\n",
    "# --- Cette partie est complexe et nécessite une compréhension profonde ---\n",
    "# --- de Prompt-to-Prompt et de l'architecture de CrossAttention de diffusers ---\n",
    "\n",
    "# 1. Créer des classes AttentionProcessor personnalisées\n",
    "class SaveAttentionProcessor:\n",
    "    def __init__(self):\n",
    "        self.attention_maps = {} # Dictionnaire pour stocker les cartes\n",
    "\n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        # ... logique pour calculer l'attention normalement ...\n",
    "        # SAUVEGARDER la carte d'attention calculée (attn.attention_probs)\n",
    "        # dans self.attention_maps avec une clé unique (ex: nom de couche, timestep)\n",
    "        # ... retourner la sortie de l'attention ...\n",
    "        return # output\n",
    "\n",
    "class InjectAttentionProcessor:\n",
    "    def __init__(self, saved_maps_source):\n",
    "        self.saved_maps = saved_maps_source # Référence aux cartes sauvegardées\n",
    "\n",
    "    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        # ... début du calcul de l'attention avec le prompt cible ...\n",
    "        # RÉCUPÉRER la carte d'attention source correspondante depuis self.saved_maps\n",
    "        # MODIFIER la carte d'attention cible en utilisant la carte source\n",
    "        # (ex: remplacement partiel basé sur les tokens, etc.)\n",
    "        # ... finir le calcul de l'attention avec la carte modifiée ...\n",
    "        # ... retourner la sortie de l'attention ...\n",
    "        return # output\n",
    "\n",
    "# 2. Modifier la boucle de sampling (Étape C)\n",
    "\n",
    "# -- Au début de la boucle de sampling --\n",
    "# att_saver = SaveAttentionProcessor()\n",
    "# unet.set_attn_processor(att_saver) # Attacher le saver\n",
    "\n",
    "# latents_source = latents.clone() # Bruit initial pour la passe source\n",
    "\n",
    "# -- Boucle sur les timesteps t --\n",
    "\n",
    "    # --- Passe Source (pour sauvegarder l'attention) ---\n",
    "    # latent_model_input_source = scheduler.scale_model_input(latents_source, t)\n",
    "    # with torch.no_grad():\n",
    "    #     # Utiliser e_opt_optimized pour la passe source\n",
    "    #     _ = unet(latent_model_input_source, t, encoder_hidden_states=e_opt_optimized).sample\n",
    "    # # Ici, att_saver.attention_maps devrait être rempli pour ce timestep\n",
    "\n",
    "    # --- Passe Cible (avec injection d'attention) ---\n",
    "    # att_injector = InjectAttentionProcessor(att_saver.attention_maps)\n",
    "    # unet.set_attn_processor(att_injector) # Attacher l'injector\n",
    "\n",
    "    # latent_model_input_target = torch.cat([latents] * 2) # Pour CFG\n",
    "    # latent_model_input_target = scheduler.scale_model_input(latent_model_input_target, t)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     # Utiliser e_tgt (ou e_bar) pour la passe cible\n",
    "    #     noise_pred = unet(latent_model_input_target, t, encoder_hidden_states=text_embeddings).sample # text_embeddings utilise e_tgt/e_bar\n",
    "\n",
    "    # --- Fin de la passe cible ---\n",
    "    # unet.set_attn_processor(None) # Détacher les processors (ou remettre les originaux)\n",
    "\n",
    "    # ... reste de la logique CFG et scheduler.step(noise_pred, t, latents) ...\n",
    "\n",
    "# --- Fin de la boucle ---\n",
    "\n",
    "# 3. Décoder l'image finale comme avant\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Chargement des composants de : CompVis/stable-diffusion-v1-4\n",
      "ERREUR lors du chargement des composants: Force download failed due to the above error.\n",
      "Vérifiez la connectivité, le chemin, le cache, ou téléchargez manuellement.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Force download failed due to the above error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/tokenizer/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1484\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1485\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders, token\u001b[38;5;241m=\u001b[39mtoken\n\u001b[1;32m   1486\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1401\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1402\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1403\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1404\u001b[0m     headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[1;32m   1405\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1406\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1407\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1408\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1409\u001b[0m )\n\u001b[1;32m   1410\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:285\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 285\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    286\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    287\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    288\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    290\u001b[0m     )\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 309\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/CompVis/stable-diffusion-v1-4/resolve/main/tokenizer/tokenizer_config.json (Request ID: Root=1-67eff479-1862d4726670d76162dcaa01;7fabdb5d-b567-4e50-a013-08e12c50e7f1)\n\nInvalid credentials in Authorization header",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVérifiez la connectivité, le chemin, le cache, ou téléchargez manuellement.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Arrêter ici si le chargement échoue\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# --- Cellule 3: Préparation Image et Texte ---\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# --- REMPLACEZ PAR VOTRE IMAGE ---\u001b[39;00m\n\u001b[1;32m     74\u001b[0m input_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/your/input_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChargement des composants de : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m CLIPTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m     text_encoder \u001b[38;5;241m=\u001b[39m CLIPTextModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m     vae \u001b[38;5;241m=\u001b[39m AutoencoderKL\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2132\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   2130\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2131\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2132\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   2133\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2134\u001b[0m         TOKENIZER_CONFIG_FILE,\n\u001b[1;32m   2135\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2136\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   2137\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   2138\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   2139\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2140\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   2141\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2142\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   2143\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   2144\u001b[0m         _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2145\u001b[0m         _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2146\u001b[0m         _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2147\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   2148\u001b[0m     )\n\u001b[1;32m   2149\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:961\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    943\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    959\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m    962\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    963\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m    965\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m    966\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    967\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    968\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m    970\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m    971\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m    972\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[1;32m    973\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    974\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    975\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m    976\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    977\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    978\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1068\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag must have been retrieved from server\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1583\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pass \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforce_download=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when offline mode is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1583\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForce download failed due to the above error.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;66;03m# No head call + couldn't find an appropriate file on disk => raise an error.\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_files_only:\n",
      "\u001b[0;31mValueError\u001b[0m: Force download failed due to the above error."
     ]
    }
   ],
   "source": [
    "# Cellule 1: Imports et Configuration Initiale\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configuration initiale (GPU, etc.)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ATTENTION: CUDA non disponible, utilisation du CPU (très lent).\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Optionnel: Patch IPv4 (si toujours nécessaire pour le téléchargement) ---\n",
    "# import socket\n",
    "# import requests.packages.urllib3.util.connection as urllib3_cn\n",
    "# import functools\n",
    "# print(\"Tentative d'application du patch IPv4...\")\n",
    "# try:\n",
    "#     if getattr(socket.getaddrinfo, '__qualname__', '') == 'patched_getaddrinfo':\n",
    "#          print(\"Le patch IPv4 semble déjà appliqué.\")\n",
    "#     else:\n",
    "#         _original_getaddrinfo = socket.getaddrinfo\n",
    "#         @functools.wraps(_original_getaddrinfo)\n",
    "#         def patched_getaddrinfo(*args, **kwargs):\n",
    "#             responses = _original_getaddrinfo(*args, **kwargs)\n",
    "#             ipv4_responses = [res for res in responses if res[0] == socket.AF_INET]\n",
    "#             return ipv4_responses if ipv4_responses else responses # Retourne original si pas d'IPv4\n",
    "#         socket.getaddrinfo = patched_getaddrinfo\n",
    "#         if not hasattr(urllib3_cn, '_original_allowed_gai_family'):\n",
    "#              urllib3_cn._original_allowed_gai_family = getattr(urllib3_cn, 'allowed_gai_family', None)\n",
    "#              urllib3_cn.allowed_gai_family = lambda: socket.AF_INET\n",
    "#         print(\"Patch IPv4 appliqué.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Échec de l'application du patch IPv4 : {e}\")\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# --- Cellule 2: Chargement des Composants Individuels ---\n",
    "# Utiliser SD 1.5 comme base standard pour Imagic/Prompt-to-Prompt\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "# OU chargez depuis un dossier local si vous avez téléchargé manuellement:\n",
    "# local_model_directory = \"chemin/vers/votre/dossier/stable-diffusion-v1-5\"\n",
    "# if not os.path.exists(local_model_directory):\n",
    "#      raise FileNotFoundError(f\"Dossier local non trouvé: {local_model_directory}\")\n",
    "# model_id = local_model_directory # Utiliser le chemin local\n",
    "\n",
    "try:\n",
    "    print(f\"Chargement des composants de : {model_id}\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\", force_download=True)\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n",
    "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\") # Utiliser DDIM comme Imagic\n",
    "    print(\"Composants chargés avec succès.\")\n",
    "\n",
    "    # Mettre les modèles en mode évaluation par défaut\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    unet.eval()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR lors du chargement des composants: {e}\")\n",
    "    print(\"Vérifiez la connectivité, le chemin, le cache, ou téléchargez manuellement.\")\n",
    "    # Arrêter ici si le chargement échoue\n",
    "    raise e\n",
    "\n",
    "# --- Cellule 3: Préparation Image et Texte ---\n",
    "# --- REMPLACEZ PAR VOTRE IMAGE ---\n",
    "input_image_path = \"path/to/your/input_image.jpg\"\n",
    "if not os.path.exists(input_image_path):\n",
    "    # Mettre une image par défaut ou lever une erreur\n",
    "    print(f\"ATTENTION: Image d'entrée non trouvée à '{input_image_path}'. Utilisation d'une image placeholder (noire).\")\n",
    "    input_image_pil = Image.new('RGB', (512, 512), color = 'black')\n",
    "    # raise FileNotFoundError(f\"Image d'entrée non trouvée: {input_image_path}\")\n",
    "else:\n",
    "    input_image_pil = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Redimensionner l'image d'entrée\n",
    "img_size = 512\n",
    "input_image_pil = input_image_pil.resize((img_size, img_size), resample=Image.LANCZOS)\n",
    "\n",
    "# --- REMPLACEZ PAR VOTRE PROMPT ---\n",
    "target_prompt = \"A photo of the input object wearing a party hat\" # Adaptez à votre image !\n",
    "\n",
    "# Fonction de prétraitement pour VAE\n",
    "def preprocess(image):\n",
    "    w, h = image.size\n",
    "    # S'assurer que la taille est multiple de 8 (pour VAE)\n",
    "    w, h = map(lambda x: x - x % 8, (w, h))\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2) # Ajouter batch dim, passer en CHW\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0 # Normaliser entre -1 et 1\n",
    "\n",
    "# Prétraiter et encoder l'image d'entrée en latente\n",
    "input_image_tensor = preprocess(input_image_pil).to(device=device, dtype=vae.dtype) # Utiliser le dtype du VAE\n",
    "with torch.no_grad():\n",
    "    # Calculer la latente initiale (z0)\n",
    "    latent_dist = vae.encode(input_image_tensor).latent_dist\n",
    "    input_latents = latent_dist.sample() * vae.config.scaling_factor # Ne pas prendre la moyenne, échantillonner\n",
    "    # Vous pouvez aussi garder mean et std si nécessaire pour des pertes VAE plus tard\n",
    "    # input_latents_mean = latent_dist.mean * vae.config.scaling_factor\n",
    "    # input_latents_logvar = latent_dist.logvar\n",
    "\n",
    "print(\"Image d'entrée préparée et encodée en latente.\")\n",
    "plt.imshow(input_image_pil)\n",
    "plt.title(\"Input Image\")\n",
    "plt.show()\n",
    "\n",
    "# Encoder le prompt cible (e_tgt)\n",
    "text_input_tgt = tokenizer(target_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    e_tgt = text_encoder(text_input_tgt.input_ids.to(device))[0].to(dtype=text_encoder.dtype) # Utiliser le dtype de l'encodeur\n",
    "\n",
    "print(\"Prompt cible encodé (e_tgt).\")\n",
    "\n",
    "\n",
    "# --- Cellule 4: Étape A - Optimisation de l'Embedding Texte (e_opt) ---\n",
    "print(\"\\n--- Début Étape A: Optimisation Embedding (e_opt) ---\")\n",
    "\n",
    "# Initialiser e_opt comme clone de e_tgt, mais entraînable\n",
    "e_opt = e_tgt.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Optimiseur pour e_opt SEULEMENT\n",
    "optimizer_embedding = optim.Adam([e_opt], lr=1e-3) # Ajuster lr si besoin\n",
    "\n",
    "# Paramètres d'optimisation\n",
    "num_optimization_steps = 100 # ~100 dans Imagic\n",
    "embedding_optimization_loss = 0 # Pour log\n",
    "\n",
    "# S'assurer que seul e_opt est entraînable\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "pbar = tqdm(range(num_optimization_steps))\n",
    "for step in pbar:\n",
    "    optimizer_embedding.zero_grad()\n",
    "\n",
    "    # Échantillonner un timestep aléatoire\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "\n",
    "    # Bruiter l'image latente d'entrée (input_latents)\n",
    "    noise = torch.randn_like(input_latents, dtype=e_opt.dtype) # Utiliser le bon dtype\n",
    "    latents_noisy = scheduler.add_noise(input_latents.to(dtype=e_opt.dtype), noise, t)\n",
    "\n",
    "    # Prédire le bruit en utilisant e_opt comme condition\n",
    "    noise_pred = unet(latents_noisy, t, encoder_hidden_states=e_opt).sample\n",
    "\n",
    "    # Calculer la perte: différence entre bruit prédit et bruit ajouté\n",
    "    loss = nn.functional.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\") # Calculer en float32 pour stabilité\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_embedding.step()\n",
    "\n",
    "    embedding_optimization_loss = loss.item()\n",
    "    pbar.set_description(f\"Step A Loss: {embedding_optimization_loss:.4f}\")\n",
    "\n",
    "print(f\"--- Fin Étape A ---\")\n",
    "# Sauvegarder l'embedding optimisé (sans gradient)\n",
    "e_opt_optimized = e_opt.clone().detach()\n",
    "\n",
    "\n",
    "# --- Cellule 5: Étape B - Fine-tuning UNet ---\n",
    "print(\"\\n--- Début Étape B: Fine-tuning UNet ---\")\n",
    "# ATTENTION: Cette étape est coûteuse en calcul et mémoire.\n",
    "# LoRA est une alternative plus efficace en pratique.\n",
    "# Ici, nous faisons un fine-tuning complet simplifié pour l'exemple.\n",
    "\n",
    "# Rendre UNet entraînable, geler le reste\n",
    "unet.requires_grad_(True)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "# e_opt_optimized n'a pas besoin de gradient\n",
    "\n",
    "# Optimiseur pour UNet SEULEMENT\n",
    "optimizer_unet = optim.Adam(unet.parameters(), lr=5e-7) # Très petit lr pour fine-tuning\n",
    "\n",
    "# Paramètres de fine-tuning\n",
    "num_finetune_steps = 100 # Réduire pour test, Imagic utilise ~1500 (très long!)\n",
    "finetune_loss = 0\n",
    "\n",
    "unet.train() # Mettre UNet en mode entraînement\n",
    "\n",
    "pbar = tqdm(range(num_finetune_steps))\n",
    "for step in pbar:\n",
    "    optimizer_unet.zero_grad()\n",
    "\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "    noise = torch.randn_like(input_latents, dtype=unet.dtype)\n",
    "    latents_noisy = scheduler.add_noise(input_latents.to(dtype=unet.dtype), noise, t)\n",
    "\n",
    "    # Prédire le bruit avec le UNet actuel et e_opt_optimized (gelé)\n",
    "    noise_pred = unet(latents_noisy, t, encoder_hidden_states=e_opt_optimized.to(dtype=unet.dtype)).sample\n",
    "\n",
    "    loss = nn.functional.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_unet.step()\n",
    "\n",
    "    finetune_loss = loss.item()\n",
    "    pbar.set_description(f\"Step B Loss: {finetune_loss:.4f}\")\n",
    "\n",
    "print(\"--- Fin Étape B ---\")\n",
    "unet.eval() # Remettre en mode évaluation\n",
    "\n",
    "# Optionnel: Sauvegarder le UNet fine-tuné\n",
    "# unet.save_pretrained(\"./unet_finetuned_imagic\")\n",
    "\n",
    "\n",
    "# --- Cellule 6: Étape C - Interpolation et Génération ---\n",
    "print(\"\\n--- Début Étape C: Interpolation et Génération ---\")\n",
    "\n",
    "eta = 0.7 # Facteur d'interpolation (entre 0 et 1) - à ajuster !\n",
    "guidance_scale = 7.5 # Force de la guidance (CFG)\n",
    "num_inference_steps = 50 # Nombre d'étapes de sampling DDIM\n",
    "\n",
    "# Calculer l'embedding interpolé\n",
    "# S'assurer que les dtypes correspondent avant l'interpolation\n",
    "e_bar = eta * e_tgt.to(dtype=e_opt_optimized.dtype) + (1 - eta) * e_opt_optimized\n",
    "\n",
    "# Préparer pour CFG (Classifier-Free Guidance)\n",
    "# Utiliser un prompt vide pour l'inconditionnel\n",
    "uncond_input = tokenizer([\"\"] * 1, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0].to(dtype=unet.dtype)\n",
    "\n",
    "# Concaténer inconditionnel et conditionnel (e_bar)\n",
    "text_embeddings_final = torch.cat([uncond_embeddings, e_bar])\n",
    "\n",
    "# Initialiser la latente de départ (bruit pur) pour l'inférence\n",
    "latents_inf = torch.randn(\n",
    "    (1, unet.config.in_channels, img_size // 8, img_size // 8), # Taille latente pour SD 1.x (512/8=64)\n",
    "    device=device,\n",
    "    dtype=unet.dtype # Utiliser le dtype de l'UNet\n",
    ")\n",
    "\n",
    "# Mettre à l'échelle le bruit initial\n",
    "latents_inf = latents_inf * scheduler.init_noise_sigma\n",
    "\n",
    "# Boucle de sampling DDIM\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "print(\"Génération de l'image éditée...\")\n",
    "pbar = tqdm(scheduler.timesteps)\n",
    "for t in pbar:\n",
    "    # Expansion des latentes pour CFG (batch size de 2)\n",
    "    latent_model_input = torch.cat([latents_inf] * 2)\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, t) # Peut nécessiter un ajustement selon le scheduler\n",
    "\n",
    "    # Prédire le bruit avec UNet fine-tuné\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings_final).sample\n",
    "\n",
    "    # Appliquer CFG\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # Calculer l'étape précédente de la latente avec le scheduler\n",
    "    latents_inf = scheduler.step(noise_pred, t, latents_inf).prev_sample\n",
    "\n",
    "print(\"Génération terminée.\")\n",
    "\n",
    "# Décoder l'image finale depuis la latente\n",
    "# Mettre à l'échelle inverse la latente\n",
    "latents_inf_scaled = 1 / vae.config.scaling_factor * latents_inf\n",
    "with torch.no_grad():\n",
    "    # Utiliser le VAE pour décoder\n",
    "    image = vae.decode(latents_inf_scaled.to(vae.dtype)).sample # Assurer le bon dtype pour VAE\n",
    "\n",
    "# Post-traitement de l'image\n",
    "image = (image / 2 + 0.5).clamp(0, 1) # Remettre entre 0 et 1\n",
    "image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0] # Passer en HWC pour affichage\n",
    "edited_image_pil = Image.fromarray((image * 255).astype(np.uint8))\n",
    "\n",
    "# Afficher l'image éditée\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(edited_image_pil)\n",
    "plt.title(f\"Edited Image (Imagic - eta={eta})\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# --- Cellule 7: (PLACEHOLDER) Phase 2 - Contrôle par Attention Croisée ---\n",
    "print(\"\\n--- Phase 2: Contrôle par Attention Croisée (Implémentation future) ---\")\n",
    "# C'est ici que vous implémenteriez la logique complexe de manipulation\n",
    "# des cartes d'attention pendant la boucle de génération (Cellule 6),\n",
    "# en vous basant sur Prompt-to-Prompt ou une technique similaire.\n",
    "# Cela nécessitera de modifier la boucle de sampling pour sauvegarder\n",
    "# et injecter/modifier les attentions.\n",
    "\n",
    "# Exemple conceptuel de génération avec attention (pseudo-code):\n",
    "# prompt_source = \"\" # Ou un prompt décrivant l'image originale\n",
    "# prompt_target = target_prompt\n",
    "# generate_with_cross_attn_control(prompt_source, prompt_target, unet_finetuned, ...)\n",
    "\n",
    "print(\"Implémentation du contrôle d'attention non réalisée dans cet exemple de base.\")\n",
    "\n",
    "\n",
    "# --- Cellule 8: (PLACEHOLDER) Phase 3 - Améliorations (GIF, Multi-Input) ---\n",
    "print(\"\\n--- Phase 3: Améliorations (Implémentation future) ---\")\n",
    "# Pour un GIF, vous mettriez la Cellule 6 dans une boucle sur eta\n",
    "# et collecteriez les images PIL dans une liste, puis utiliseriez imageio.mimsave.\n",
    "\n",
    "# Pour le multi-input, la stratégie reste à définir (très avancé)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Chargement des composants de : CompVis/stable-diffusion-v1-4\n",
      "Erreur lors du chargement des composants : Can't load tokenizer for 'CompVis/stable-diffusion-v1-4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CompVis/stable-diffusion-v1-4' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer.\n",
      "Vérifiez la connectivité, le chemin, le cache, ou téléchargez manuellement.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'CompVis/stable-diffusion-v1-4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CompVis/stable-diffusion-v1-4' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVérifiez la connectivité, le chemin, le cache, ou téléchargez manuellement.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Arrêter ici si le chargement échoue\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# --- Cellule 3: Préparation Image et Texte ---\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# --- REMPLACEZ PAR VOTRE IMAGE ---\u001b[39;00m\n\u001b[1;32m     75\u001b[0m input_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/your/input_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# À remplacer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChargement des composants de : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m CLIPTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m     text_encoder \u001b[38;5;241m=\u001b[39m CLIPTextModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m     vae \u001b[38;5;241m=\u001b[39m AutoencoderKL\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2197\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'CompVis/stable-diffusion-v1-4'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'CompVis/stable-diffusion-v1-4' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "# Cellule 1: Imports et Configuration Initiale\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Configuration initiale (GPU, etc.)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ATTENTION: GPU non disponible, l'exécution sera très lente!\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Optionnel: Patch IPv4 (si toujours nécessaire pour le téléchargement) ---\n",
    "# import socket\n",
    "# import requests.packages.urllib3.util.connection as urllib3_cn\n",
    "# import functools\n",
    "# print(\"Tentative d'application du patch IPv4...\")\n",
    "# try:\n",
    "#     if getattr(socket.getaddrinfo, '__qualname__', '') == 'patched_getaddrinfo':\n",
    "#          print(\"Le patch IPv4 semble déjà appliqué.\")\n",
    "#     else:\n",
    "#         _original_getaddrinfo = socket.getaddrinfo\n",
    "#         @functools.wraps(_original_getaddrinfo)\n",
    "#         def patched_getaddrinfo(*args, **kwargs):\n",
    "#             responses = _original_getaddrinfo(*args, **kwargs)\n",
    "#             ipv4_responses = [res for res in responses if res[0] == socket.AF_INET]\n",
    "#             return ipv4_responses if ipv4_responses else responses # Retourne original si pas d'IPv4\n",
    "#         socket.getaddrinfo = patched_getaddrinfo\n",
    "#         if not hasattr(urllib3_cn, '_original_allowed_gai_family'):\n",
    "#              urllib3_cn._original_allowed_gai_family = getattr(urllib3_cn, 'allowed_gai_family', None)\n",
    "#              urllib3_cn.allowed_gai_family = lambda: socket.AF_INET\n",
    "#         print(\"Patch IPv4 appliqué.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Échec de l'application du patch IPv4 : {e}\")\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# --- Cellule 2: Chargement des Composants Individuels ---\n",
    "# Utiliser SD 1.4 comme base pour Imagic\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "# OU chargez depuis un dossier local si vous avez téléchargé manuellement:\n",
    "# local_model_directory = \"chemin/vers/votre/dossier/stable-diffusion-v1-4\"\n",
    "# if not os.path.exists(local_model_directory):\n",
    "#      raise FileNotFoundError(f\"Dossier local non trouvé: {local_model_directory}\")\n",
    "# model_id = local_model_directory # Utiliser le chemin local\n",
    "\n",
    "try:\n",
    "    print(f\"Chargement des composants de : {model_id}\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n",
    "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Mettre les modèles en mode évaluation\n",
    "    text_encoder.eval()\n",
    "    vae.eval()\n",
    "    unet.eval()\n",
    "    \n",
    "    print(\"Tous les composants chargés avec succès!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement des composants : {e}\")\n",
    "    print(\"Vérifiez la connectivité, le chemin, le cache, ou téléchargez manuellement.\")\n",
    "    # Arrêter ici si le chargement échoue\n",
    "    raise e\n",
    "\n",
    "# --- Cellule 3: Préparation Image et Texte ---\n",
    "# --- REMPLACEZ PAR VOTRE IMAGE ---\n",
    "input_image_path = \"path/to/your/input_image.jpg\"  # À remplacer\n",
    "if not os.path.exists(input_image_path):\n",
    "    print(f\"Image introuvable: {input_image_path}\")\n",
    "    # Option: utiliser une image de démonstration si disponible\n",
    "    input_image_path = \"/home/antoine/genai_project/assets/demo_image.jpg\"  # À ajuster\n",
    "    if not os.path.exists(input_image_path):\n",
    "        raise FileNotFoundError(\"Aucune image trouvée. Spécifiez un chemin valide.\")\n",
    "else:\n",
    "    print(f\"Image trouvée: {input_image_path}\")\n",
    "\n",
    "# Charger l'image d'entrée\n",
    "input_image_pil = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Redimensionner l'image d'entrée\n",
    "img_size = 512\n",
    "input_image_pil = input_image_pil.resize((img_size, img_size), resample=Image.LANCZOS)\n",
    "\n",
    "# --- REMPLACEZ PAR VOTRE PROMPT ---\n",
    "target_prompt = \"A photo of the input object wearing a party hat\"  # Adaptez à votre image !\n",
    "\n",
    "# Fonction de prétraitement pour VAE\n",
    "def preprocess(image):\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Prétraiter et encoder l'image d'entrée en latente\n",
    "input_image_tensor = preprocess(input_image_pil).to(device=device, dtype=vae.dtype)  # Utiliser le dtype du VAE\n",
    "with torch.no_grad():\n",
    "    # Encoder l'image en latente avec le VAE\n",
    "    latent_dist = vae.encode(input_image_tensor)\n",
    "    latent_input = latent_dist.latent_dist.sample() * 0.18215  # Facteur d'échelle du VAE\n",
    "\n",
    "print(\"Image d'entrée préparée et encodée en latente.\")\n",
    "plt.imshow(input_image_pil)\n",
    "plt.title(\"Input Image\")\n",
    "plt.show()\n",
    "\n",
    "# Encoder le prompt cible (e_tgt)\n",
    "text_input_tgt = tokenizer(target_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, \n",
    "                           truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    e_tgt = text_encoder(text_input_tgt.input_ids.to(device))[0]  # Embeddings du texte cible\n",
    "\n",
    "print(\"Prompt cible encodé (e_tgt).\")\n",
    "\n",
    "\n",
    "# --- Cellule 4: Étape A - Optimisation de l'Embedding Texte (e_opt) ---\n",
    "print(\"\\n--- Début Étape A: Optimisation Embedding (e_opt) ---\")\n",
    "\n",
    "# Initialiser e_opt comme clone de e_tgt, mais entraînable\n",
    "e_opt = e_tgt.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Optimiseur pour e_opt SEULEMENT\n",
    "optimizer_embedding = optim.Adam([e_opt], lr=1e-3)  # Ajuster lr si besoin\n",
    "\n",
    "# Paramètres d'optimisation\n",
    "num_optimization_steps = 100  # ~100 dans Imagic\n",
    "embedding_optimization_loss = 0  # Pour log\n",
    "\n",
    "# S'assurer que seul e_opt est entraînable\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "pbar = tqdm(range(num_optimization_steps))\n",
    "for step in pbar:\n",
    "    # Réinitialiser les gradients\n",
    "    optimizer_embedding.zero_grad()\n",
    "    \n",
    "    # Échantillonner un timestep t aléatoire\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "    \n",
    "    # Ajouter du bruit à la latente originale selon t\n",
    "    noise = torch.randn_like(latent_input)\n",
    "    noisy_latent = scheduler.add_noise(latent_input, noise, t)\n",
    "    \n",
    "    # Prédiction de bruit par l'UNet avec l'embedding optimisable\n",
    "    noise_pred = unet(noisy_latent, t, encoder_hidden_states=e_opt).sample\n",
    "    \n",
    "    # Calculer la perte MSE entre le bruit prédit et le bruit ajouté\n",
    "    loss = nn.MSELoss()(noise_pred, noise)\n",
    "    \n",
    "    # Rétropropagation et optimisation\n",
    "    loss.backward()\n",
    "    optimizer_embedding.step()\n",
    "    \n",
    "    # Mise à jour de la barre de progression\n",
    "    embedding_optimization_loss = loss.item()\n",
    "    pbar.set_description(f\"Optimisation embedding - Loss: {embedding_optimization_loss:.4f}\")\n",
    "\n",
    "print(f\"--- Fin Étape A ---\")\n",
    "# Sauvegarder l'embedding optimisé (sans gradient)\n",
    "e_opt_optimized = e_opt.clone().detach()\n",
    "\n",
    "\n",
    "# --- Cellule 5: Étape B - Fine-tuning UNet ---\n",
    "print(\"\\n--- Début Étape B: Fine-tuning UNet ---\")\n",
    "# ATTENTION: Cette étape est coûteuse en calcul et mémoire.\n",
    "# LoRA est une alternative plus efficace en pratique.\n",
    "# Ici, nous faisons un fine-tuning complet simplifié pour l'exemple.\n",
    "\n",
    "# Rendre UNet entraînable, geler le reste\n",
    "unet.requires_grad_(True)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "# e_opt_optimized n'a pas besoin de gradient\n",
    "\n",
    "# Optimiseur pour UNet SEULEMENT\n",
    "optimizer_unet = optim.Adam(unet.parameters(), lr=5e-7)  # Très petit lr pour fine-tuning\n",
    "\n",
    "# Paramètres de fine-tuning\n",
    "num_finetune_steps = 100  # Réduire pour test, Imagic utilise ~1500 (très long!)\n",
    "finetune_loss = 0\n",
    "\n",
    "unet.train()  # Mettre UNet en mode entraînement\n",
    "\n",
    "pbar = tqdm(range(num_finetune_steps))\n",
    "for step in pbar:\n",
    "    # Réinitialiser les gradients\n",
    "    optimizer_unet.zero_grad()\n",
    "    \n",
    "    # Échantillonner un timestep t aléatoire\n",
    "    t = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "    \n",
    "    # Ajouter du bruit à la latente originale selon t\n",
    "    noise = torch.randn_like(latent_input)\n",
    "    noisy_latent = scheduler.add_noise(latent_input, noise, t)\n",
    "    \n",
    "    # Prédiction de bruit par l'UNet avec l'embedding optimisé fixe\n",
    "    noise_pred = unet(noisy_latent, t, encoder_hidden_states=e_opt_optimized).sample\n",
    "    \n",
    "    # Calculer la perte MSE entre le bruit prédit et le bruit ajouté\n",
    "    loss = nn.MSELoss()(noise_pred, noise)\n",
    "    \n",
    "    # Rétropropagation et optimisation\n",
    "    loss.backward()\n",
    "    optimizer_unet.step()\n",
    "    \n",
    "    # Mise à jour de la barre de progression\n",
    "    finetune_loss = loss.item()\n",
    "    pbar.set_description(f\"Fine-tuning UNet - Loss: {finetune_loss:.4f}\")\n",
    "\n",
    "print(\"--- Fin Étape B ---\")\n",
    "unet.eval()  # Remettre en mode évaluation\n",
    "\n",
    "# Optionnel: Sauvegarder le UNet fine-tuné\n",
    "# unet.save_pretrained(\"./unet_finetuned_imagic\")\n",
    "\n",
    "\n",
    "# --- Cellule 6: Étape C - Interpolation et Génération ---\n",
    "print(\"\\n--- Début Étape C: Interpolation et Génération ---\")\n",
    "\n",
    "eta = 0.7  # Facteur d'interpolation (entre 0 et 1) - à ajuster !\n",
    "guidance_scale = 7.5  # Force de la guidance (CFG)\n",
    "num_inference_steps = 50  # Nombre d'étapes de sampling DDIM\n",
    "\n",
    "# Calculer l'embedding interpolé\n",
    "# S'assurer que les dtypes correspondent avant l'interpolation\n",
    "e_bar = eta * e_tgt.to(dtype=e_opt_optimized.dtype) + (1 - eta) * e_opt_optimized\n",
    "\n",
    "# Préparer pour CFG (Classifier-Free Guidance)\n",
    "# Utiliser un prompt vide pour l'inconditionnel\n",
    "uncond_input = tokenizer([\"\"], padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "\n",
    "# Concaténer inconditionnel et conditionnel (e_bar)\n",
    "text_embeddings_final = torch.cat([uncond_embeddings, e_bar])\n",
    "\n",
    "# Initialiser la latente de départ (bruit pur) pour l'inférence\n",
    "latents_inf = torch.randn(\n",
    "    (1, unet.config.in_channels, img_size // 8, img_size // 8),\n",
    "    device=device,\n",
    "    dtype=unet.dtype)\n",
    "\n",
    "# Mettre à l'échelle le bruit initial\n",
    "latents_inf = latents_inf * scheduler.init_noise_sigma\n",
    "\n",
    "# Boucle de sampling DDIM\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "print(\"Génération de l'image éditée...\")\n",
    "pbar = tqdm(scheduler.timesteps)\n",
    "for t in pbar:\n",
    "    # Dupliquer les latentes pour la guidance (inconditionnel et conditionnel)\n",
    "    latent_model_input = torch.cat([latents_inf] * 2)\n",
    "    \n",
    "    # Définir le timestep actuel\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "    \n",
    "    # Prédire le bruit résiduel\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings_final).sample\n",
    "    \n",
    "    # Séparer les prédictions conditionnelles et inconditionnelles\n",
    "    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "    \n",
    "    # Appliquer la guidance: pred_noise = uncond + guidance_scale * (cond - uncond)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "    \n",
    "    # Étape de débruitage DDIM\n",
    "    latents_inf = scheduler.step(noise_pred, t, latents_inf).prev_sample\n",
    "\n",
    "print(\"Génération terminée.\")\n",
    "\n",
    "# Décoder l'image finale depuis la latente\n",
    "with torch.no_grad():\n",
    "    latents_inf = 1 / 0.18215 * latents_inf  # Mise à l'échelle inverse du VAE\n",
    "    image = vae.decode(latents_inf).sample\n",
    "\n",
    "# Conversion en image PIL\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "image = (image * 255).round().astype(\"uint8\")[0]\n",
    "final_image = Image.fromarray(image)\n",
    "\n",
    "# Afficher l'image originale et l'image éditée\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(input_image_pil)\n",
    "ax[0].set_title(\"Image originale\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(final_image)\n",
    "ax[1].set_title(f\"Image éditée: {target_prompt}\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarder l'image générée\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "final_image.save(\"./output/edited_image.png\")\n",
    "print(\"Image éditée sauvegardée dans ./output/edited_image.png\")\n",
    "\n",
    "\n",
    "# --- Cellule 7: Visualisation de l'attention croisée ---\n",
    "print(\"\\n--- Visualisation de l'attention croisée ---\")\n",
    "\n",
    "# Fonction pour extraire les cartes d'attention croisée de l'UNet\n",
    "def get_cross_attention(unet, latents, t, text_embeddings):\n",
    "    # Activation hooks pour capturer l'attention\n",
    "    attention_maps = []\n",
    "    \n",
    "    def attention_hook(module, input, output):\n",
    "        attention_map = output[1]  # Forme: [batch, heads, seq_len, dim]\n",
    "        # Moyenne sur les têtes d'attention\n",
    "        attention_map = attention_map.mean(1)  # [batch, seq_len, dim]\n",
    "        attention_maps.append(attention_map)\n",
    "    \n",
    "    # Enregistrer les hooks sur les modules d'attention croisée\n",
    "    hooks = []\n",
    "    for name, module in unet.named_modules():\n",
    "        if \"attn2\" in name:  # attn2 est l'attention croisée texte-image\n",
    "            hooks.append(module.register_forward_hook(attention_hook))\n",
    "    \n",
    "    # Passer les latentes à travers l'UNet\n",
    "    with torch.no_grad():\n",
    "        _ = unet(latents, t, encoder_hidden_states=text_embeddings)\n",
    "    \n",
    "    # Retirer les hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "        \n",
    "    return attention_maps\n",
    "\n",
    "# Générer des cartes d'attention pour une timestep spécifique\n",
    "t_idx = len(scheduler.timesteps) // 2  # Timestep médiane\n",
    "t_attention = scheduler.timesteps[t_idx]\n",
    "\n",
    "# Préparer l'entrée du modèle\n",
    "latent_input_viz = scheduler.scale_model_input(latents_inf, t_attention)\n",
    "\n",
    "# Récupérer les cartes d'attention\n",
    "attention_maps = get_cross_attention(unet, latent_input_viz, t_attention, e_bar.unsqueeze(0))\n",
    "\n",
    "# Visualiser les cartes d'attention pour les tokens importants\n",
    "# Trouver l'index des tokens principaux dans le prompt\n",
    "tokens = tokenizer.tokenize(target_prompt)\n",
    "print(f\"Tokens du prompt: {tokens}\")\n",
    "\n",
    "# Sélectionner quelques tokens intéressants pour la visualisation\n",
    "selected_token_indices = [1, 2, 3]  # À ajuster selon les tokens significatifs\n",
    "token_attention_weights = {}\n",
    "\n",
    "# Prendre la dernière carte d'attention (généralement la plus significative)\n",
    "last_attention_map = attention_maps[-1][0]  # [seq_len, dim]\n",
    "\n",
    "# Redimensionner pour correspondre à la taille de l'image\n",
    "resolution = img_size // 8\n",
    "for idx in selected_token_indices:\n",
    "    if idx < last_attention_map.shape[0]:\n",
    "        token = tokens[idx-1] if idx > 0 and idx-1 < len(tokens) else f\"token_{idx}\"\n",
    "        attention_weights = last_attention_map[idx].reshape(resolution, resolution)\n",
    "        attention_weights = attention_weights.cpu().numpy()\n",
    "        token_attention_weights[token] = attention_weights\n",
    "\n",
    "# Visualiser les cartes d'attention pour les tokens sélectionnés\n",
    "n_tokens = len(token_attention_weights)\n",
    "if n_tokens > 0:\n",
    "    fig, axes = plt.subplots(1, n_tokens + 1, figsize=(15, 5))\n",
    "    \n",
    "    # Image originale\n",
    "    axes[0].imshow(input_image_pil)\n",
    "    axes[0].set_title(\"Image originale\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Cartes d'attention\n",
    "    for i, (token, weights) in enumerate(token_attention_weights.items(), 1):\n",
    "        axes[i].imshow(weights, cmap='inferno')\n",
    "        axes[i].set_title(f\"Attention: {token}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Aucune carte d'attention disponible pour visualisation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
