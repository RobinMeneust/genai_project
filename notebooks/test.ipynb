{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()  # same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 18:53:11.570240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744131191.613672 1097120 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744131191.632287 1097120 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-08 18:53:11.673889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de l'appareil: cuda\n",
      "Chargement du modèle stable-diffusion-v1-5/stable-diffusion-v1-5...\n",
      "Tentative de chargement du modèle complet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'use_auth_token': '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'} are not expected by StableDiffusionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be47e37d8e14b038ab9e87b83bec356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 635\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;66;03m# Exemple d'utilisation simple\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m     imagic \u001b[38;5;241m=\u001b[39m ImageEditingWithImagic(model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstable-diffusion-v1-5/stable-diffusion-v1-5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Chemin vers l'image d'entrée (à remplacer par votre chemin)\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     input_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/antoine/genai_project/assets/dog.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 127\u001b[0m, in \u001b[0;36mImageEditingWithImagic.__init__\u001b[0;34m(self, model_id, verbose, use_auth_token)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTentative de chargement du modèle complet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpipe_kwargs\n\u001b[0;32m--> 127\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py:461\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb:\n\u001b[0;32m--> 461\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device, dtype)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    464\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    468\u001b[0m ):\n\u001b[1;32m    469\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    475\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/diffusers/models/modeling_utils.py:1077\u001b[0m, in \u001b[0;36mModelMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_bitsandbytes_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `to()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m         )\n\u001b[0;32m-> 1077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1161\u001b[0m         device,\n\u001b[1;32m   1162\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1163\u001b[0m         non_blocking,\n\u001b[1;32m   1164\u001b[0m     )\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Imports nécessaires\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from typing import List, Optional, Tuple, Union, Dict\n",
    "\n",
    "# Imports pour diffusers - Correction des importations\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel\n",
    "from diffusers.models.attention_processor import Attention\n",
    "# Supprimer l'importation incorrecte:\n",
    "# from diffusers.models.cross_attention import CrossAttention\n",
    "\n",
    "# Pour l'optimisation d'embedding et le fine-tuning\n",
    "import torch.optim as optim\n",
    "\n",
    "# Définir l'appareil\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de l'appareil: {device}\")\n",
    "\n",
    "# 1. Classe d'attention avec capture des cartes d'attention\n",
    "class AttentionStore:\n",
    "    \"\"\"\n",
    "    Stocke les cartes d'attention pour l'analyse et la visualisation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.store = {}\n",
    "        \n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if is_cross:\n",
    "            key = f\"{place_in_unet}_cross\"\n",
    "            if key not in self.store:\n",
    "                self.store[key] = []\n",
    "            self.store[key].append(attn)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.store = {}\n",
    "    \n",
    "    def get_average_attention(self):\n",
    "        result = {}\n",
    "        for key, attention_maps in self.store.items():\n",
    "            average_map = torch.stack(attention_maps).mean(0)\n",
    "            result[key] = average_map\n",
    "        return result\n",
    "\n",
    "# 2. Processeur d'attention pour capturer les cartes\n",
    "class AttentionProcessor(nn.Module):\n",
    "    def __init__(self, attn_store):\n",
    "        super().__init__()\n",
    "        self.attn_store = attn_store\n",
    "        \n",
    "    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "        query = attn.to_q(hidden_states)\n",
    "        \n",
    "        is_cross = encoder_hidden_states is not None\n",
    "        \n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "            \n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "        \n",
    "        query = attn.head_to_batch_dim(query)\n",
    "        key = attn.head_to_batch_dim(key)\n",
    "        value = attn.head_to_batch_dim(value)\n",
    "        \n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        \n",
    "        # Stocker les cartes d'attention\n",
    "        if is_cross and self.attn_store is not None:\n",
    "            self.attn_store(attention_probs, is_cross, attn.place_in_unet)\n",
    "            \n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# 3. Classe principale pour Imagic avec Cross-Attention\n",
    "class ImageEditingWithImagic:\n",
    "    # In the ImageEditingWithImagic class, update the __init__ method:\n",
    "    \n",
    "    def __init__(self, model_id=\"stable-diffusion-v1-5/stable-diffusion-v1-5\", verbose=True, use_auth_token=True):\n",
    "        self.model_id = model_id\n",
    "        self.verbose = verbose\n",
    "        self.attention_store = AttentionStore()\n",
    "        \n",
    "        # Charger le pipeline de Stable Diffusion\n",
    "        if self.verbose:\n",
    "            print(f\"Chargement du modèle {self.model_id}...\")\n",
    "        \n",
    "        # Ensure all imports are available regardless of execution path\n",
    "        from diffusers import AutoencoderKL, DDIMScheduler\n",
    "        from transformers import CLIPTextModel, CLIPTokenizer, CLIPFeatureExtractor\n",
    "        \n",
    "        # Configuration pour le chargement du modèle\n",
    "        pipe_kwargs = {\n",
    "            \"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            \"safety_checker\": None,\n",
    "            \"requires_safety_checker\": False\n",
    "        }\n",
    "        \n",
    "        # Ajouter le token d'authentification si nécessaire\n",
    "        if use_auth_token and isinstance(use_auth_token, bool):\n",
    "            import huggingface_hub\n",
    "            pipe_kwargs[\"use_auth_token\"] = huggingface_hub.HfFolder.get_token()\n",
    "        elif use_auth_token:\n",
    "            pipe_kwargs[\"use_auth_token\"] = use_auth_token\n",
    "    \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Tentative de chargement du modèle complet...\")\n",
    "            self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                self.model_id,\n",
    "                **pipe_kwargs\n",
    "            ).to(device)\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"Erreur lors du chargement complet: {str(e)}\")\n",
    "                print(\"Tentative de chargement des composants individuellement...\")\n",
    "            \n",
    "            try:\n",
    "                # Charger les composants individuellement\n",
    "                tokenizer = CLIPTokenizer.from_pretrained(self.model_id, subfolder=\"tokenizer\")\n",
    "                text_encoder = CLIPTextModel.from_pretrained(self.model_id, subfolder=\"text_encoder\", **pipe_kwargs)\n",
    "                vae = AutoencoderKL.from_pretrained(self.model_id, subfolder=\"vae\", **pipe_kwargs)\n",
    "                unet = UNet2DConditionModel.from_pretrained(self.model_id, subfolder=\"unet\", **pipe_kwargs)\n",
    "                scheduler = DDIMScheduler.from_pretrained(self.model_id, subfolder=\"scheduler\")\n",
    "                \n",
    "                # Tenter de charger feature_extractor s'il existe\n",
    "                try:\n",
    "                    feature_extractor = CLIPFeatureExtractor.from_pretrained(self.model_id, subfolder=\"feature_extractor\")\n",
    "                except Exception:\n",
    "                    if self.verbose:\n",
    "                        print(\"Feature extractor non trouvé, initialisation à None\")\n",
    "                    feature_extractor = None\n",
    "                \n",
    "                # Créer le pipeline avec tous les composants\n",
    "                self.pipe = StableDiffusionPipeline(\n",
    "                    vae=vae,\n",
    "                    text_encoder=text_encoder,\n",
    "                    tokenizer=tokenizer,\n",
    "                    unet=unet,\n",
    "                    scheduler=scheduler,\n",
    "                    safety_checker=None,\n",
    "                    feature_extractor=feature_extractor,\n",
    "                    requires_safety_checker=False\n",
    "                ).to(device)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(\"Modèle chargé avec succès par composants individuels\")\n",
    "                    \n",
    "            except Exception as component_error:\n",
    "                raise ValueError(\n",
    "                    f\"Impossible de charger le modèle {self.model_id}. \"\n",
    "                    f\"Erreur: {str(e)}. \"\n",
    "                    f\"Erreur de chargement des composants: {str(component_error)}. \"\n",
    "                    \"Essayez avec un autre modèle comme 'runwayml/stable-diffusion-v1-5'.\"\n",
    "                )\n",
    "        \n",
    "        # Configurer le scheduler pour DDIM (meilleur pour l'inversion)\n",
    "        self.pipe.scheduler = DDIMScheduler.from_config(self.pipe.scheduler.config)\n",
    "        \n",
    "        # Faire une référence aux sous-modèles pour un accès facile\n",
    "        self.vae = self.pipe.vae\n",
    "        self.unet = self.pipe.unet\n",
    "        self.text_encoder = self.pipe.text_encoder\n",
    "        self.tokenizer = self.pipe.tokenizer\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Modèle chargé avec succès!\")\n",
    "    \n",
    "    def register_attention_control(self):\n",
    "        \"\"\"\n",
    "        Enregistre le processeur d'attention personnalisé pour capturer les cartes d'attention\n",
    "        \"\"\"\n",
    "        attn_procs = {}\n",
    "        \n",
    "        # Modification: ne pas chercher CrossAttention spécifiquement, mais trouver\n",
    "        # les modules d'attention cross-attention dans UNet\n",
    "        for name, module in self.unet.named_modules():\n",
    "            # Rechercher les modules d'attention (généralement nommés attn2 pour cross-attention)\n",
    "            if \"attn2\" in name or (isinstance(module, Attention) and getattr(module, 'is_cross_attention', False)):\n",
    "                attn_procs[name] = AttentionProcessor(self.attention_store)\n",
    "        \n",
    "        # Utiliser le mécanisme d'attention processors compatible avec votre version de diffusers\n",
    "        # Pour les versions récentes:\n",
    "        try:\n",
    "            self.unet.set_attn_processor(attn_procs)\n",
    "        except AttributeError:\n",
    "            # Fallback pour d'anciennes versions\n",
    "            for name, processor in attn_procs.items():\n",
    "                for n, m in self.unet.named_modules():\n",
    "                    if n == name:\n",
    "                        m.processor = processor\n",
    "        \n",
    "    # Le reste de la classe reste inchangé\n",
    "    def preprocess_image(self, image, target_size=512):\n",
    "        \"\"\"\n",
    "        Prétraite l'image pour l'encoder avec le VAE\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        \n",
    "        # Redimensionnement si nécessaire\n",
    "        if image.width != target_size or image.height != target_size:\n",
    "            image = image.resize((target_size, target_size), Image.LANCZOS)\n",
    "            \n",
    "        # Convertir en tenseur\n",
    "        image = np.array(image) / 255.0\n",
    "        image = torch.from_numpy(image).float().permute(2, 0, 1)\n",
    "        # Normaliser entre -1 et 1\n",
    "        image = 2.0 * image - 1.0\n",
    "        \n",
    "        # Check if the model expects 4 channels\n",
    "        in_channels = getattr(self.vae.config, \"in_channels\", 3)\n",
    "        \n",
    "        # Add alpha channel if needed\n",
    "        if in_channels == 4 and image.shape[0] == 3:\n",
    "            # Add an alpha channel (fully opaque)\n",
    "            alpha_channel = torch.ones((1, image.shape[1], image.shape[2]), device=image.device)\n",
    "            image = torch.cat([image, alpha_channel], dim=0)\n",
    "        \n",
    "        # Ajouter la dimension batch\n",
    "        image = image.unsqueeze(0).to(device=device, dtype=self.vae.dtype)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def encode_image_to_latent(self, image):\n",
    "        \"\"\"\n",
    "        Encode l'image en latent avec le VAE\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            latent = self.vae.encode(image).latent_dist.sample()\n",
    "            latent = 0.18215 * latent  # Mise à l'échelle pour SD\n",
    "        return latent\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        \"\"\"\n",
    "        Encode le texte en embedding\n",
    "        \"\"\"\n",
    "        text_input = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_embedding = self.text_encoder(\n",
    "                text_input.input_ids.to(device)\n",
    "            )[0]\n",
    "        \n",
    "        return text_embedding\n",
    "    \n",
    "    def step_a_optimize_text_embedding(self, input_image, target_text, num_steps=100, lr=1e-3):\n",
    "        \"\"\"\n",
    "        Étape A: Optimiser l'embedding du texte pour l'image d'entrée\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\n--- Étape A: Optimisation de l'embedding pour '{target_text}' ---\")\n",
    "        \n",
    "        # Encoder l'image en latent\n",
    "        if isinstance(input_image, torch.Tensor):\n",
    "            latent_input = input_image\n",
    "        else:\n",
    "            preprocessed_image = self.preprocess_image(input_image)\n",
    "            latent_input = self.encode_image_to_latent(preprocessed_image)\n",
    "        \n",
    "        # Encoder le texte cible\n",
    "        target_embedding = self.encode_text(target_text)\n",
    "        \n",
    "        # Créer un embedding optimisable\n",
    "        optimizable_embedding = target_embedding.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        # Optimizer uniquement pour l'embedding\n",
    "        optimizer = optim.Adam([optimizable_embedding], lr=lr)\n",
    "        \n",
    "        # Fixer les paramètres du modèle\n",
    "        self.vae.requires_grad_(False)\n",
    "        self.text_encoder.requires_grad_(False)\n",
    "        self.unet.requires_grad_(False)\n",
    "        \n",
    "        # Boucle d'optimisation\n",
    "        pbar = tqdm(range(num_steps), desc=\"Optimisation embedding\")\n",
    "        for step in pbar:\n",
    "            # Réinitialiser les gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Échantillonner un timestep aléatoire\n",
    "            t = torch.randint(0, self.pipe.scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "            \n",
    "            # Ajouter du bruit à la latente originale selon t\n",
    "            noise = torch.randn_like(latent_input)\n",
    "            noisy_latent = self.pipe.scheduler.add_noise(latent_input, noise, t)\n",
    "            \n",
    "            # Prédiction de bruit par l'UNet avec l'embedding optimisable\n",
    "            noise_pred = self.unet(noisy_latent, t, encoder_hidden_states=optimizable_embedding).sample\n",
    "            \n",
    "            # Calculer la perte MSE\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            # Rétropropagation et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Mise à jour de la barre de progression\n",
    "            pbar.set_description(f\"Optimisation embedding - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Détacher l'embedding optimisé pour ne plus avoir de gradient\n",
    "        optimized_embedding = optimizable_embedding.clone().detach()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"--- Fin Étape A: Embedding optimisé avec succès ---\")\n",
    "            \n",
    "        return optimized_embedding, latent_input, target_embedding\n",
    "    def step_b_finetune_unet(self, latent_input, optimized_embedding, num_steps=200, lr=5e-7):\n",
    "        \"\"\"\n",
    "        Étape B: Fine-tuner l'UNet pour améliorer la fidélité à l'image d'entrée\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"\\n--- Étape B: Fine-tuning de l'UNet ---\")\n",
    "        \n",
    "        # Configurer l'UNet pour le fine-tuning\n",
    "        self.unet.requires_grad_(True)\n",
    "        self.text_encoder.requires_grad_(False)\n",
    "        self.vae.requires_grad_(False)\n",
    "        \n",
    "        # Optimizer pour l'UNet\n",
    "        optimizer = optim.Adam(self.unet.parameters(), lr=lr)\n",
    "        \n",
    "        # Boucle de fine-tuning\n",
    "        pbar = tqdm(range(num_steps), desc=\"Fine-tuning UNet\")\n",
    "        for step in pbar:\n",
    "            # Réinitialiser les gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Échantillonner un timestep aléatoire\n",
    "            t = torch.randint(0, self.pipe.scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "            \n",
    "            # Ajouter du bruit à la latente selon t\n",
    "            noise = torch.randn_like(latent_input)\n",
    "            noisy_latent = self.pipe.scheduler.add_noise(latent_input, noise, t)\n",
    "            \n",
    "            # Prédiction avec l'embedding optimisé\n",
    "            noise_pred = self.unet(noisy_latent, t, encoder_hidden_states=optimized_embedding).sample\n",
    "            \n",
    "            # Calculer la perte MSE\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "            \n",
    "            # Rétropropagation et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Mise à jour de la barre de progression\n",
    "            pbar.set_description(f\"Fine-tuning UNet - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Remettre l'UNet en mode évaluation\n",
    "        self.unet.eval()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"--- Fin Étape B: UNet fine-tuné avec succès ---\")\n",
    "            \n",
    "        return self.unet\n",
    "    \n",
    "    def step_c_generate_with_attention_control(self, target_embedding, optimized_embedding, \n",
    "                                           eta=0.7, guidance_scale=7.5, num_inference_steps=50,\n",
    "                                           height=512, width=512):\n",
    "        \"\"\"\n",
    "        Étape C: Interpolation et génération avec contrôle d'attention\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"\\n--- Étape C: Génération avec Cross-Attention Control ---\")\n",
    "        \n",
    "        # Calculer l'embedding interpolé\n",
    "        interpolated_embedding = eta * target_embedding + (1 - eta) * optimized_embedding\n",
    "        \n",
    "        # Activer la capture d'attention\n",
    "        self.register_attention_control()\n",
    "        self.attention_store.reset()\n",
    "        \n",
    "        # Préparer pour CFG (Classifier-Free Guidance)\n",
    "        uncond_input = self.tokenizer(\n",
    "            [\"\"], \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.tokenizer.model_max_length, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(device))[0]\n",
    "        \n",
    "        # Concaténer inconditionnel et conditionnel\n",
    "        text_embeddings = torch.cat([uncond_embeddings, interpolated_embedding])\n",
    "        \n",
    "        # Générer l'image\n",
    "        with torch.no_grad():\n",
    "            latents = torch.randn((1, self.unet.config.in_channels, height // 8, width // 8)).to(device)\n",
    "            latents = latents * self.pipe.scheduler.init_noise_sigma\n",
    "            \n",
    "            # Configurer le scheduler\n",
    "            self.pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "            \n",
    "            # Boucle de sampling\n",
    "            for i, t in enumerate(tqdm(self.pipe.scheduler.timesteps, desc=\"Génération\")):\n",
    "                latent_model_input = torch.cat([latents] * 2)\n",
    "                \n",
    "                # Échelle des latentes pour le timestep actuel\n",
    "                latent_model_input = self.pipe.scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "                \n",
    "                # Prédire le bruit résiduel avec attention\n",
    "                noise_pred = self.unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=text_embeddings\n",
    "                ).sample\n",
    "                \n",
    "                # Appliquer la guidance\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                \n",
    "                # Étape de débruitage\n",
    "                latents = self.pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            # Décoder l'image finale\n",
    "            latents = 1 / 0.18215 * latents\n",
    "            image = self.vae.decode(latents).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "            image = (image * 255).round().astype(\"uint8\")[0]\n",
    "            final_image = Image.fromarray(image)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"--- Fin Étape C: Image générée avec succès ---\")\n",
    "        \n",
    "        # Récupérer les cartes d'attention moyenne\n",
    "        attention_maps = self.attention_store.get_average_attention()\n",
    "        \n",
    "        return final_image, attention_maps\n",
    "    \n",
    "    def visualize_attention_maps(self, original_image, generated_image, attention_maps, num_maps=4):\n",
    "        \"\"\"\n",
    "        Visualise les cartes d'attention pour comprendre le processus d'édition\n",
    "        \"\"\"\n",
    "        # Sélectionner les cartes d'attention du milieu de l'UNet (souvent les plus significatives)\n",
    "        mid_block_maps = None\n",
    "        up_block_maps = None\n",
    "        \n",
    "        for key, maps in attention_maps.items():\n",
    "            if 'mid_block' in key:\n",
    "                mid_block_maps = maps[0]  # premier batch, forme: [heads, seq_len, dim]\n",
    "                # Moyenne sur les têtes d'attention\n",
    "                mid_block_maps = mid_block_maps.mean(0)  # [seq_len, dim]\n",
    "            elif 'up_blocks.3' in key:  # Utiliser une des dernières couches up_block\n",
    "                up_block_maps = maps[0]\n",
    "                up_block_maps = up_block_maps.mean(0)\n",
    "        \n",
    "        if mid_block_maps is None and up_block_maps is None:\n",
    "            print(\"Aucune carte d'attention trouvée pour visualisation\")\n",
    "            return\n",
    "        \n",
    "        attention_map = mid_block_maps if mid_block_maps is not None else up_block_maps\n",
    "        \n",
    "        # Résolution pour redimensionner les cartes d'attention\n",
    "        resolution = 16 if mid_block_maps is not None else 32\n",
    "        \n",
    "        # Créer une figure avec les images originale/éditée et les cartes d'attention\n",
    "        fig, axs = plt.subplots(1, 3 + num_maps, figsize=(18, 5))\n",
    "        \n",
    "        # Image originale\n",
    "        axs[0].imshow(original_image)\n",
    "        axs[0].set_title(\"Image originale\")\n",
    "        axs[0].axis(\"off\")\n",
    "        \n",
    "        # Image générée\n",
    "        axs[1].imshow(generated_image)\n",
    "        axs[1].set_title(\"Image éditée\")\n",
    "        axs[1].axis(\"off\")\n",
    "        \n",
    "        # Carte d'attention moyenne globale\n",
    "        avg_attn = attention_map.mean(0).reshape(resolution, resolution).cpu().numpy()\n",
    "        axs[2].imshow(avg_attn, cmap='inferno')\n",
    "        axs[2].set_title(\"Attention moyenne\")\n",
    "        axs[2].axis(\"off\")\n",
    "        \n",
    "        # Cartes d'attention individuelles pour certains tokens\n",
    "        for i in range(num_maps):\n",
    "            if i < attention_map.shape[0]:\n",
    "                idx = i + 1  # Premiers tokens (souvent les plus significatifs)\n",
    "                attn = attention_map[idx].reshape(resolution, resolution).cpu().numpy()\n",
    "                axs[3 + i].imshow(attn, cmap='inferno')\n",
    "                axs[3 + i].set_title(f\"Token {idx}\")\n",
    "                axs[3 + i].axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def edit_image(self, input_image, target_text, \n",
    "                  steps_a=100, lr_a=1e-3,\n",
    "                  steps_b=200, lr_b=5e-7,\n",
    "                  eta=0.7, guidance_scale=7.5, inference_steps=50):\n",
    "        \"\"\"\n",
    "        Méthode principale pour éditer une image avec Imagic\n",
    "        \"\"\"\n",
    "        # 1. Prétraiter l'image\n",
    "        if isinstance(input_image, str):\n",
    "            input_image_pil = Image.open(input_image).convert(\"RGB\")\n",
    "            preprocessed_image = self.preprocess_image(input_image)\n",
    "        elif isinstance(input_image, Image.Image):\n",
    "            input_image_pil = input_image\n",
    "            preprocessed_image = self.preprocess_image(input_image)\n",
    "        else:\n",
    "            preprocessed_image = input_image\n",
    "            # Reconstruire pour visualisation\n",
    "            with torch.no_grad():\n",
    "                rec_latents = self.vae.encode(preprocessed_image).latent_dist.mean\n",
    "                rec_latents = 1 / 0.18215 * rec_latents\n",
    "                rec_image = self.vae.decode(rec_latents).sample\n",
    "                rec_image = (rec_image / 2 + 0.5).clamp(0, 1)\n",
    "                rec_image = rec_image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "                rec_image = (rec_image * 255).round().astype(\"uint8\")[0]\n",
    "                input_image_pil = Image.fromarray(rec_image)\n",
    "        \n",
    "        # 2. Étape A: Optimiser l'embedding de texte\n",
    "        optimized_embedding, latent_input, target_embedding = self.step_a_optimize_text_embedding(\n",
    "            preprocessed_image, target_text, num_steps=steps_a, lr=lr_a\n",
    "        )\n",
    "        \n",
    "        # 3. Étape B: Fine-tuner l'UNet\n",
    "        finetuned_unet = self.step_b_finetune_unet(\n",
    "            latent_input, optimized_embedding, num_steps=steps_b, lr=lr_b\n",
    "        )\n",
    "        \n",
    "        # 4. Étape C: Générer avec contrôle d'attention\n",
    "        generated_image, attention_maps = self.step_c_generate_with_attention_control(\n",
    "            target_embedding, optimized_embedding, \n",
    "            eta=eta, guidance_scale=guidance_scale, \n",
    "            num_inference_steps=inference_steps\n",
    "        )\n",
    "        \n",
    "        # 5. Visualiser les cartes d'attention\n",
    "        fig = self.visualize_attention_maps(input_image_pil, generated_image, attention_maps)\n",
    "        \n",
    "        # 6. Sauvegarder\n",
    "        os.makedirs(\"./output\", exist_ok=True)\n",
    "        generated_image.save(\"./output/edited_image.png\")\n",
    "        \n",
    "        return {\n",
    "            \"original_image\": input_image_pil,\n",
    "            \"edited_image\": generated_image,\n",
    "            \"attention_maps\": attention_maps,\n",
    "            \"optimized_embedding\": optimized_embedding,\n",
    "            \"target_embedding\": target_embedding,\n",
    "            \"figure\": fig\n",
    "        }\n",
    "\n",
    "# 4. Génération de séquences d'images (extension proposée)\n",
    "class ImageSequenceGenerator(ImageEditingWithImagic):\n",
    "    \"\"\"\n",
    "    Extension pour générer des séquences d'images (comme proposé dans la proposition)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id=\"stabilityai/stable-diffusion-2\"):\n",
    "        super().__init__(model_id)\n",
    "    \n",
    "    def generate_sequence(self, input_image, target_text, num_frames=8, \n",
    "                          start_eta=0.1, end_eta=0.9, \n",
    "                          **kwargs):\n",
    "        \"\"\"\n",
    "        Génère une séquence d'images interpolées entre l'image originale et l'édition complète\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Génération d'une séquence de {num_frames} images ---\")\n",
    "        \n",
    "        # D'abord faire les étapes A et B (les mêmes pour toute la séquence)\n",
    "        preprocessed_image = self.preprocess_image(input_image)\n",
    "        optimized_embedding, latent_input, target_embedding = self.step_a_optimize_text_embedding(\n",
    "            preprocessed_image, target_text, **kwargs\n",
    "        )\n",
    "        \n",
    "        _ = self.step_b_finetune_unet(latent_input, optimized_embedding, **kwargs)\n",
    "        \n",
    "        # Générer les images avec différentes valeurs d'eta\n",
    "        eta_values = np.linspace(start_eta, end_eta, num_frames)\n",
    "        sequence = []\n",
    "        \n",
    "        for i, eta in enumerate(tqdm(eta_values, desc=\"Génération de séquence\")):\n",
    "            print(f\"Frame {i+1}/{num_frames}, eta={eta:.2f}\")\n",
    "            \n",
    "            image, _ = self.step_c_generate_with_attention_control(\n",
    "                target_embedding, optimized_embedding, eta=eta, **kwargs\n",
    "            )\n",
    "            \n",
    "            sequence.append(image)\n",
    "        \n",
    "        # Créer un GIF si souhaité\n",
    "        os.makedirs(\"./output\", exist_ok=True)\n",
    "        sequence[0].save('./output/sequence.gif',\n",
    "                   save_all=True, append_images=sequence[1:], optimize=False, \n",
    "                   duration=200, loop=0)\n",
    "        \n",
    "        # Visualiser la séquence\n",
    "        fig, axs = plt.subplots(1, len(sequence), figsize=(20, 4))\n",
    "        for i, img in enumerate(sequence):\n",
    "            if len(sequence) > 1:\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].set_title(f\"eta={eta_values[i]:.2f}\")\n",
    "                axs[i].axis(\"off\")\n",
    "            else:\n",
    "                axs.imshow(img)\n",
    "                axs.set_title(f\"eta={eta_values[i]:.2f}\")\n",
    "                axs.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple d'utilisation simple\n",
    "    imagic = ImageEditingWithImagic(model_id=\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
    "    \n",
    "    # Chemin vers l'image d'entrée (à remplacer par votre chemin)\n",
    "    input_image_path = \"/home/antoine/genai_project/assets/dog.jpg\"\n",
    "    \n",
    "    # Texte cible pour l'édition\n",
    "    target_text = \"A photo of a dog wearing a party hat\"\n",
    "    \n",
    "    # Éditer l'image\n",
    "    results = imagic.edit_image(\n",
    "        input_image_path,\n",
    "        target_text,\n",
    "        steps_a=100,\n",
    "        steps_b=200,\n",
    "        eta=0.7\n",
    "    )\n",
    "    \n",
    "    # Pour générer une séquence\n",
    "    seq_generator = ImageSequenceGenerator(model_id=\"stabilityai/stable-diffusion-2-1\")\n",
    "    \n",
    "    sequence = seq_generator.generate_sequence(\n",
    "        input_image_path,\n",
    "        target_text,\n",
    "        num_frames=5,\n",
    "        steps_a=100,  # Réduire pour la démo\n",
    "        steps_b=100   # Réduire pour la démo\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de l'appareil: cuda\n",
      "\n",
      "--- Initialisation pour Édition Simple ---\n",
      "Tentative de chargement des composants pour 'stable-diffusion-v1-5/stable-diffusion-v1-5'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/anaconda3/lib/python3.12/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composants chargés avec succès!\n",
      "\n",
      "--- Démarrage de l'édition ---\n",
      "\n",
      "--- Étape A: Optimisation de l'embedding pour 'A dog with a blue hat' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimisation embedding - Loss: 0.0405: 100%|██████████| 100/100 [00:37<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fin Étape A ---\n",
      "\n",
      "--- Étape B: Fine-tuning de l'UNet ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning UNet - Loss: 0.0199:   2%|▏         | 3/150 [02:29<1:48:32, 44.30s/it]"
     ]
    }
   ],
   "source": [
    "# Imports nécessaires (garder ceux du début de votre fichier)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from typing import List, Optional, Tuple, Union, Dict\n",
    "\n",
    "# Imports pour diffusers\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, UNet2DConditionModel, AutoencoderKL\n",
    "from diffusers.models.attention_processor import Attention\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPFeatureExtractor # S'assurer que tout est importé\n",
    "\n",
    "# Pour l'optimisation d'embedding et le fine-tuning\n",
    "import torch.optim as optim\n",
    "import huggingface_hub # Pour gérer le token\n",
    "\n",
    "# Définir l'appareil\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de l'appareil: {device}\")\n",
    "\n",
    "# --- Classes AttentionStore et AttentionProcessor (Garder tel quel) ---\n",
    "class AttentionStore:\n",
    "    def __init__(self): self.store = {}\n",
    "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        if is_cross:\n",
    "            key = f\"{place_in_unet}_cross\"; self.store.setdefault(key, []).append(attn)\n",
    "    def reset(self): self.store = {}\n",
    "    def get_average_attention(self):\n",
    "        return {k: torch.stack(v).mean(0) for k, v in self.store.items()}\n",
    "\n",
    "class AttentionProcessor(nn.Module):\n",
    "    def __init__(self, attn_store, place_in_unet):\n",
    "        super().__init__()\n",
    "        self.attn_store = attn_store\n",
    "        self.place_in_unet = place_in_unet # Ajouter place_in_unet ici\n",
    "    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, attention_mask=None):\n",
    "        batch_size, sequence_length, _ = hidden_states.shape\n",
    "        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "        query = attn.to_q(hidden_states)\n",
    "        is_cross = encoder_hidden_states is not None\n",
    "        encoder_hidden_states = encoder_hidden_states if is_cross else hidden_states\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "        # --- Correction potentielle pour cross-attention norm ---\n",
    "        # if attn.norm_cross and is_cross: # Vérifier si l'attribut existe\n",
    "        #    encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "        # --- fin correction ---\n",
    "        query = attn.head_to_batch_dim(query); key = attn.head_to_batch_dim(key); value = attn.head_to_batch_dim(value)\n",
    "        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n",
    "        if is_cross and self.attn_store is not None:\n",
    "            # Utiliser place_in_unet stocké\n",
    "            self.attn_store(attention_probs.cpu(), is_cross, self.place_in_unet) # Envoyer sur CPU pour économiser VRAM ?\n",
    "        hidden_states = torch.bmm(attention_probs, value)\n",
    "        hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "        hidden_states = attn.to_out[0](hidden_states); hidden_states = attn.to_out[1](hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class ControlledAttentionProcessor(nn.Module):\n",
    "    def __init__(self, tokenizer, target_prompt_tokens): # Besoin du tokenizer et des tokens cibles\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_prompt_tokens = target_prompt_tokens\n",
    "        # Identifier les indices des tokens à modifier (ex: \"party\", \"hat\")\n",
    "        # Ceci est SIMPLISTE, une meilleure méthode est nécessaire\n",
    "        self.edit_token_indices = self._get_edit_token_indices(target_prompt_tokens)\n",
    "\n",
    "    def _get_edit_token_indices(self, tokens):\n",
    "        # Logique pour trouver les indices des mots ajoutés/modifiés\n",
    "        # Exemple très basique:\n",
    "        indices = []\n",
    "        edit_words = [\"party\", \"hat\"] # À rendre dynamique\n",
    "        for i, token_id in enumerate(tokens[0]): # Supposer batch size 1\n",
    "            word = self.tokenizer.decode([token_id])\n",
    "            if any(edit_word in word for edit_word in edit_words):\n",
    "                 indices.append(i)\n",
    "        return indices\n",
    "\n",
    "    # __call__ doit être modifié pour accepter e_source et e_target\n",
    "    # via cross_attention_kwargs ou une autre méthode\n",
    "    def __call__(self, attn: Attention, hidden_states, encoder_hidden_states=None, attention_mask=None, **cross_attention_kwargs):\n",
    "         # Récupérer e_source et e_target des kwargs\n",
    "         e_source = cross_attention_kwargs.get(\"e_source\")\n",
    "         e_target = cross_attention_kwargs.get(\"e_target\")\n",
    "         if e_source is None or e_target is None:\n",
    "              # Fallback: utiliser l'embedding passé normalement si un seul est fourni\n",
    "              return # ... logique d'attention standard ...\n",
    "\n",
    "         # --- Calculer Q, K, V pour source ET target ---\n",
    "         query = attn.to_q(hidden_states)\n",
    "         key_source = attn.to_k(e_source); value_source = attn.to_v(e_source)\n",
    "         key_target = attn.to_k(e_target); value_target = attn.to_v(e_target)\n",
    "\n",
    "         # Reshape pour les têtes\n",
    "         query = attn.head_to_batch_dim(query)\n",
    "         key_source = attn.head_to_batch_dim(key_source); value_source = attn.head_to_batch_dim(value_source)\n",
    "         key_target = attn.head_to_batch_dim(key_target); value_target = attn.head_to_batch_dim(value_target)\n",
    "\n",
    "         # Calculer les probas d'attention pour les deux\n",
    "         attn_probs_source = attn.get_attention_scores(query, key_source, attention_mask)\n",
    "         attn_probs_target = attn.get_attention_scores(query, key_target, attention_mask)\n",
    "\n",
    "         # --- Injection/Modification ---\n",
    "         attn_probs_modified = attn_probs_target.clone()\n",
    "         # Pour tous les tokens SAUF ceux à éditer, copier depuis la source\n",
    "         for k in range(attn_probs_target.shape[-1]): # Itérer sur les tokens K\n",
    "             if k not in self.edit_token_indices:\n",
    "                  attn_probs_modified[:, :, k] = attn_probs_source[:, :, k]\n",
    "\n",
    "         # --- Calcul final avec value_target et attn_probs_modified ---\n",
    "         hidden_states = torch.bmm(attn_probs_modified, value_target)\n",
    "         hidden_states = attn.batch_to_head_dim(hidden_states)\n",
    "         hidden_states = attn.to_out[0](hidden_states); hidden_states = attn.to_out[1](hidden_states)\n",
    "         return hidden_states\n",
    "\n",
    "# --- Classe principale Modifiée ---\n",
    "class ImageEditingWithImagic:\n",
    "\n",
    "    def __init__(self, model_id_or_path=\"stabilityai/stable-diffusion-v1-5\", verbose=True, use_auth_token=None):\n",
    "        self.model_id_or_path = model_id_or_path\n",
    "        self.verbose = verbose\n",
    "        self.attention_store = AttentionStore()\n",
    "\n",
    "        # Configuration pour le chargement (commune aux deux méthodes)\n",
    "        load_kwargs = {\n",
    "            \"torch_dtype\": torch.float32,\n",
    "            # Les safety checkers peuvent être lourds et sont souvent désactivés pour l'édition\n",
    "            # Décommentez si vous les voulez absolument, mais ils peuvent nécessiter plus de VRAM/temps\n",
    "            # \"safety_checker\": StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-safety-checker\"),\n",
    "            # \"requires_safety_checker\": True\n",
    "        }\n",
    "\n",
    "        # Gestion du token (optionnel, utile pour modèles privés/gated)\n",
    "        token = None\n",
    "        if use_auth_token is True:\n",
    "            token = huggingface_hub.HfFolder.get_token()\n",
    "        elif isinstance(use_auth_token, str):\n",
    "            token = use_auth_token\n",
    "        if token:\n",
    "             load_kwargs[\"use_auth_token\"] = token # Ajouté dynamiquement si un token est fourni\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Tentative de chargement des composants pour '{self.model_id_or_path}'...\")\n",
    "\n",
    "        # --- PRIORITÉ AU CHARGEMENT DES COMPOSANTS ---\n",
    "        try:\n",
    "            # Charger les composants individuellement\n",
    "            self.tokenizer = CLIPTokenizer.from_pretrained(self.model_id_or_path, subfolder=\"tokenizer\", **load_kwargs)\n",
    "            self.text_encoder = CLIPTextModel.from_pretrained(self.model_id_or_path, subfolder=\"text_encoder\", **load_kwargs).to(device)\n",
    "            self.vae = AutoencoderKL.from_pretrained(self.model_id_or_path, subfolder=\"vae\", **load_kwargs).to(device)\n",
    "            self.unet = UNet2DConditionModel.from_pretrained(self.model_id_or_path, subfolder=\"unet\", **load_kwargs).to(device)\n",
    "            self.scheduler = DDIMScheduler.from_pretrained(self.model_id_or_path, subfolder=\"scheduler\") # Pas besoin de dtype ici\n",
    "\n",
    "            # Feature extractor est optionnel et souvent non nécessaire si pas de safety checker\n",
    "            try:\n",
    "                self.feature_extractor = CLIPFeatureExtractor.from_pretrained(self.model_id_or_path, subfolder=\"feature_extractor\", **load_kwargs)\n",
    "            except Exception:\n",
    "                if self.verbose:\n",
    "                    print(\"Feature extractor non trouvé ou non nécessaire, initialisation à None.\")\n",
    "                self.feature_extractor = None\n",
    "\n",
    "            # Mettre les modèles en mode évaluation par défaut\n",
    "            self.vae.eval()\n",
    "            self.text_encoder.eval()\n",
    "            self.unet.eval()\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"Composants chargés avec succès!\")\n",
    "\n",
    "        except Exception as component_error:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nERREUR lors du chargement des composants: {str(component_error)}\")\n",
    "                print(\"Causes possibles: chemin local incorrect, ID Hugging Face invalide, problème réseau, cache corrompu.\")\n",
    "                print(\"-> Vérifiez le chemin/ID: \", self.model_id_or_path)\n",
    "                print(\"-> Vérifiez la connexion réseau / pare-feu / VPN.\")\n",
    "                print(\"-> Essayez de supprimer le cache: rm -rf ~/.cache/huggingface\")\n",
    "                print(\"-> Si chemin local, vérifiez qu'il contient les sous-dossiers: tokenizer, text_encoder, vae, unet, scheduler\")\n",
    "            raise component_error # Relancer l'erreur après les messages d'aide\n",
    "\n",
    "\n",
    "    def register_attention_control(self):\n",
    "        \"\"\"\n",
    "        Enregistre le processeur d'attention personnalisé UNIQUEMENT pour les couches de cross-attention.\n",
    "        \"\"\"\n",
    "        attn_procs = {}\n",
    "        num_registered = 0\n",
    "        try:\n",
    "            for name, module in self.unet.named_modules():\n",
    "                # Cibler spécifiquement les modules d'attention qui sont probablement de la cross-attention\n",
    "                # dans l'architecture SDv1/v2 (souvent nommés 'attn2')\n",
    "                if \"attn2\" in name and isinstance(module, Attention):\n",
    "                    # Créer une instance de processeur pour CETTE couche spécifique\n",
    "                    # Lui passer son nom pour le stockage\n",
    "                    processor_instance = AttentionProcessor(self.attention_store, name)\n",
    "                    # Assigner directement à l'attribut 'processor' du module\n",
    "                    # C'est souvent plus compatible entre versions que set_attn_processor global\n",
    "                    module.processor = processor_instance\n",
    "                    num_registered += 1\n",
    "\n",
    "            if self.verbose:\n",
    "                if num_registered > 0:\n",
    "                    print(f\"Processeurs d'attention enregistrés pour {num_registered} couches de cross-attention via assignation directe.\")\n",
    "                else:\n",
    "                    print(\"AVERTISSEMENT: Aucune couche de cross-attention ('attn2') trouvée pour enregistrer les processeurs.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"Erreur lors de l'assignation directe des processeurs d'attention: {e}\")\n",
    "                print(\"La capture d'attention pourrait ne pas fonctionner.\")\n",
    "\n",
    "    # --- Garder les méthodes preprocess_image, encode_image_to_latent, encode_text ---\n",
    "    # --- Garder les méthodes step_a, step_b, step_c (vérifier les dtypes à l'intérieur si float16 est utilisé) ---\n",
    "    # --- Garder visualize_attention_maps et edit_image ---\n",
    "    # ... (Collez ici les méthodes inchangées de votre code original) ...\n",
    "    # --- Assurez-vous que les appels à self.pipe.* sont remplacés par self.vae.*, self.unet.* etc. ---\n",
    "    # Exemple de correction dans step_a:\n",
    "    # noisy_latent = self.scheduler.add_noise(...) au lieu de self.pipe.scheduler...\n",
    "    # noise_pred = self.unet(...) au lieu de self.pipe.unet...\n",
    "    # Pareil dans step_c etc. J'ai mis à jour step_a et step_c ci-dessous. step_b utilise déjà self.unet\n",
    "\n",
    "    def preprocess_image(self, image, target_size=512):\n",
    "        if isinstance(image, str): image = Image.open(image).convert(\"RGB\")\n",
    "        if image.width != target_size or image.height != target_size: image = image.resize((target_size, target_size), Image.LANCZOS)\n",
    "        image = np.array(image).astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        image = 2.0 * image - 1.0\n",
    "        image = image.unsqueeze(0).to(device=device, dtype=self.vae.dtype)\n",
    "        return image\n",
    "\n",
    "    def encode_image_to_latent(self, image):\n",
    "        with torch.no_grad(): latent = self.vae.encode(image).latent_dist.sample() * self.vae.config.scaling_factor\n",
    "        return latent\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        text_input = self.tokenizer(text, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad(): text_embedding = self.text_encoder(text_input.input_ids.to(device))[0]\n",
    "        return text_embedding.to(dtype=self.text_encoder.dtype) # Assurer le bon dtype\n",
    "\n",
    "    def step_a_optimize_text_embedding(self, input_image, target_text, num_steps=100, lr=1e-3):\n",
    "        if self.verbose: print(f\"\\n--- Étape A: Optimisation de l'embedding pour '{target_text}' ---\")\n",
    "        if isinstance(input_image, torch.Tensor): preprocessed_image = input_image # Assume already preprocessed if tensor\n",
    "        else: preprocessed_image = self.preprocess_image(input_image)\n",
    "        latent_input = self.encode_image_to_latent(preprocessed_image)\n",
    "        target_embedding = self.encode_text(target_text)\n",
    "        optimizable_embedding = target_embedding.clone().detach().requires_grad_(True)\n",
    "        optimizer = optim.Adam([optimizable_embedding], lr=lr)\n",
    "        self.vae.requires_grad_(False); self.text_encoder.requires_grad_(False); self.unet.requires_grad_(False)\n",
    "        pbar = tqdm(range(num_steps), desc=\"Optimisation embedding\")\n",
    "        for step in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            t = torch.randint(0, self.scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "            noise = torch.randn_like(latent_input)\n",
    "            # Utiliser self.scheduler directement\n",
    "            noisy_latent = self.scheduler.add_noise(latent_input.to(noise.dtype), noise, t) # Assurer compatibilité dtype\n",
    "            noise_pred = self.unet(noisy_latent, t, encoder_hidden_states=optimizable_embedding.to(self.unet.dtype)).sample\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "            loss.backward();\n",
    "            torch.nn.utils.clip_grad_norm_(optimizable_embedding, 1.0)\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f\"Optimisation embedding - Loss: {loss.item():.4f}\")\n",
    "        optimized_embedding = optimizable_embedding.clone().detach()\n",
    "        if self.verbose: print(f\"--- Fin Étape A ---\")\n",
    "        return optimized_embedding, latent_input, target_embedding\n",
    "\n",
    "    def step_b_finetune_unet(self, latent_input, optimized_embedding, num_steps=200, lr=5e-7):\n",
    "        if self.verbose: print(\"\\n--- Étape B: Fine-tuning de l'UNet ---\")\n",
    "        self.unet.requires_grad_(True); self.text_encoder.requires_grad_(False); self.vae.requires_grad_(False)\n",
    "        optimizer = optim.Adam(self.unet.parameters(), lr=lr) # Optimiser seulement UNet\n",
    "        self.unet.train() # Mettre en mode train\n",
    "        pbar = tqdm(range(num_steps), desc=\"Fine-tuning UNet\")\n",
    "        for step in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            t = torch.randint(0, self.scheduler.config.num_train_timesteps, (1,), device=device).long()\n",
    "            noise = torch.randn_like(latent_input)\n",
    "            noisy_latent = self.scheduler.add_noise(latent_input.to(noise.dtype), noise, t)\n",
    "            noise_pred = self.unet(noisy_latent, t, encoder_hidden_states=optimized_embedding.to(self.unet.dtype)).sample\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "            loss.backward();\n",
    "            torch.nn.utils.clip_grad_norm_(self.unet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f\"Fine-tuning UNet - Loss: {loss.item():.4f}\")\n",
    "        self.unet.eval() # Remettre en mode eval\n",
    "        if self.verbose: print(\"--- Fin Étape B ---\")\n",
    "        return self.unet # Retourner le UNet fine-tuné (même si modifié en place)\n",
    "\n",
    "    def step_c_generate_with_attention_control(self, target_embedding, optimized_embedding,\n",
    "                                           eta=0.7, guidance_scale=7.5, num_inference_steps=50,\n",
    "                                           height=512, width=512, generator=None): # Ajouter generator pour reproductibilité\n",
    "        if self.verbose: print(\"\\n--- Étape C: Génération ---\")\n",
    "        interpolated_embedding = eta * target_embedding.to(optimized_embedding.dtype) + (1 - eta) * optimized_embedding\n",
    "        self.register_attention_control(); self.attention_store.reset()\n",
    "        uncond_input = self.tokenizer([\"\"] * 1, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad(): uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(device))[0].to(self.unet.dtype)\n",
    "        text_embeddings = torch.cat([uncond_embeddings, interpolated_embedding])\n",
    "        latents = torch.randn((1, self.unet.config.in_channels, height // 8, width // 8), generator=generator, device=device, dtype=self.unet.dtype) # Utiliser generator\n",
    "        # Utiliser self.scheduler directement\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        latents = latents * self.scheduler.init_noise_sigma # Utiliser le scheduler chargé\n",
    "        pbar_c = tqdm(self.scheduler.timesteps, desc=\"Génération\")\n",
    "        for i, t in enumerate(pbar_c):\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "            with torch.no_grad(): noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        latents = 1 / self.vae.config.scaling_factor * latents # Utiliser le VAE chargé\n",
    "        with torch.no_grad(): image = self.vae.decode(latents.to(self.vae.dtype)).sample\n",
    "        image = (image / 2 + 0.5).clamp(0, 1).cpu().permute(0, 2, 3, 1).float().numpy()[0] # Utiliser float() avant numpy\n",
    "        final_image = Image.fromarray((image * 255).round().astype(\"uint8\"))\n",
    "        if self.verbose: print(\"--- Fin Étape C ---\")\n",
    "        attention_maps = self.attention_store.get_average_attention()\n",
    "        return final_image, attention_maps\n",
    "\n",
    "    def visualize_attention_maps(self, original_image, generated_image, attention_maps, num_token_maps=4, sequence_length=77): # Ajouter sequence_length\n",
    "        \"\"\"\n",
    "        Visualise les cartes d'attention pour comprendre le processus d'édition\n",
    "        \"\"\"\n",
    "        if not attention_maps:\n",
    "             if self.verbose: print(\"Aucune carte d'attention à visualiser.\")\n",
    "             return None\n",
    "\n",
    "        # Essayer de trouver une carte de cross-attention pertinente des couches supérieures/milieu\n",
    "        key_to_show = None\n",
    "        possible_keys = sorted([k for k in attention_maps if 'cross' in k and ('up_blocks.1' in k or 'up_blocks.2' in k or 'mid_block' in k)]) # Prioriser up/mid blocks\n",
    "        if not possible_keys:\n",
    "             possible_keys = sorted(list(attention_maps.keys())) # Prendre n'importe laquelle si pas de cross pertinente trouvée\n",
    "        if possible_keys:\n",
    "            key_to_show = possible_keys[-1] # Prendre une des dernières couches trouvées\n",
    "        else:\n",
    "             if self.verbose: print(\"Aucune clé de carte d'attention trouvée.\")\n",
    "             return None\n",
    "\n",
    "        # Récupérer la carte moyenne (sur les étapes de diffusion) pour cette couche\n",
    "        # La forme devrait être [Heads, Q, K] après mean(0) dans get_average\n",
    "        attn_map_step_avg = attention_maps[key_to_show]\n",
    "\n",
    "        # Vérifier la forme attendue [Heads, QueryLength, KeyLength]\n",
    "        if attn_map_step_avg.ndim != 3:\n",
    "             if self.verbose: print(f\"Forme inattendue pour attn_map_step_avg ('{key_to_show}'): {attn_map_step_avg.shape}. Attendu 3 dimensions.\")\n",
    "             return None\n",
    "\n",
    "        num_heads, query_len, key_len = attn_map_step_avg.shape\n",
    "\n",
    "        # Vérifier si la longueur de clé correspond à la longueur de séquence attendue\n",
    "        if key_len != sequence_length:\n",
    "            if self.verbose:\n",
    "                print(f\"Attention ('{key_to_show}'): La longueur de clé ({key_len}) ne correspond pas à sequence_length ({sequence_length}). La visualisation des tokens pourrait être incorrecte.\")\n",
    "            # Ajuster sequence_length pour correspondre à la carte réelle si nécessaire pour la boucle\n",
    "            sequence_length = key_len\n",
    "\n",
    "        # Essayer de deviner la résolution spatiale Q = H*W\n",
    "        resolution_squared = query_len\n",
    "        resolution = int(np.sqrt(resolution_squared))\n",
    "        if resolution * resolution != resolution_squared:\n",
    "            if self.verbose: print(f\"Impossible de déterminer la résolution spatiale pour '{key_to_show}' à partir de query_len={query_len}.\")\n",
    "            # Essayer une résolution commune si la clé le suggère (ex: mid_block -> 8*8?) - logique plus complexe nécessaire\n",
    "            # Pour l'instant, on abandonne si la résolution n'est pas carrée.\n",
    "            # Les couches d'attention dans SD ont généralement des résolutions spatiales carrées (64x64, 32x32, 16x16, 8x8)\n",
    "            print(f\"-> Tentative avec résolution 64 (pour 4096), 32 (pour 1024), 16 (pour 256), 8 (pour 64)\")\n",
    "            possible_res = {64: 4096, 32: 1024, 16: 256, 8: 64}\n",
    "            found_res = False\n",
    "            for res_guess, q_len_guess in possible_res.items():\n",
    "                 if query_len == q_len_guess:\n",
    "                      resolution = res_guess\n",
    "                      found_res = True\n",
    "                      print(f\"-> Résolution spatiale déduite : {resolution}x{resolution}\")\n",
    "                      break\n",
    "            if not found_res:\n",
    "                 print(\"-> Échec de la déduction de la résolution spatiale.\")\n",
    "                 return None # Abandonner si on ne trouve pas\n",
    "\n",
    "        if self.verbose: print(f\"Visualisation de l'attention pour {key_to_show}, résolution {resolution}x{resolution}, {key_len} tokens.\")\n",
    "\n",
    "        # --- Option A: Attention Spatiale Moyenne (Reçue) ---\n",
    "        # Moyenne sur les têtes ET les tokens texte (dimension K)\n",
    "        # Forme résultante: [Q] qui peut être reshapée en [H, W]\n",
    "        try:\n",
    "            attn_spatial_avg = attn_map_step_avg.mean(dim=(0, 2)).reshape(resolution, resolution).cpu().numpy()\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du calcul/reshape de l'attention spatiale moyenne: {e}\")\n",
    "            attn_spatial_avg = np.zeros((resolution, resolution)) # Placeholder en cas d'erreur\n",
    "\n",
    "        # --- Option B: Attention pour des Tokens Spécifiques (Donnée) ---\n",
    "        # Moyenne sur les têtes (dimension 0) -> [Q, K]\n",
    "        try:\n",
    "            attn_head_avg = attn_map_step_avg.mean(dim=0) # Shape: [Q, K]\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la moyenne sur les têtes: {e}\")\n",
    "            attn_head_avg = torch.zeros((query_len, key_len)) # Placeholder\n",
    "\n",
    "        # Nombre de cartes de tokens à afficher\n",
    "        num_token_maps_to_show = min(num_token_maps, sequence_length)\n",
    "\n",
    "        # Créer la figure\n",
    "        # +1 pour l'image originale, +1 pour l'éditée, +1 pour la spatiale moyenne, +N pour les tokens\n",
    "        fig, axs = plt.subplots(1, 3 + num_token_maps_to_show, figsize=(18, 4)) # Ajuster figsize si besoin\n",
    "        axs = axs.flatten() # Assurer que axs est toujours indexable\n",
    "\n",
    "        # Afficher les images\n",
    "        axs[0].imshow(original_image); axs[0].set_title(\"Originale\"); axs[0].axis(\"off\")\n",
    "        axs[1].imshow(generated_image); axs[1].set_title(\"Éditée\"); axs[1].axis(\"off\")\n",
    "\n",
    "        # Afficher la carte d'activation spatiale moyenne\n",
    "        im_spatial = axs[2].imshow(attn_spatial_avg, cmap='viridis'); axs[2].set_title(\"Attn Spatiale Moyenne\"); axs[2].axis(\"off\")\n",
    "        # fig.colorbar(im_spatial, ax=axs[2], shrink=0.6) # Optionnel\n",
    "\n",
    "        # Afficher les cartes par token\n",
    "        for i in range(num_token_maps_to_show):\n",
    "             token_idx = i # Afficher les premiers tokens (0: <bos>, 1: premier mot, ...)\n",
    "             try:\n",
    "                 # Sélectionner le token, reshaper la dimension spatiale Q en HxW\n",
    "                 attn_token = attn_head_avg[:, token_idx].reshape(resolution, resolution).cpu().numpy()\n",
    "             except Exception as e:\n",
    "                 print(f\"Erreur lors du reshape pour token {token_idx}: {e}\")\n",
    "                 attn_token = np.zeros((resolution, resolution)) # Placeholder\n",
    "\n",
    "             ax_idx = 3 + i\n",
    "             if ax_idx < len(axs): # Vérifier si l'axe existe\n",
    "                 im_token = axs[ax_idx].imshow(attn_token, cmap='viridis'); axs[ax_idx].set_title(f\"Token {token_idx}\"); axs[ax_idx].axis(\"off\")\n",
    "                 # fig.colorbar(im_token, ax=axs[ax_idx], shrink=0.6) # Optionnel\n",
    "\n",
    "        # Cacher les axes restants si moins de cartes sont affichées que prévu\n",
    "        for j in range(3 + num_token_maps_to_show, len(axs)):\n",
    "             axs[j].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout(); plt.show()\n",
    "        return fig\n",
    "\n",
    "\n",
    "    def edit_image(self, input_image, target_text,\n",
    "                  steps_a=100, lr_a=1e-3,\n",
    "                  steps_b=200, lr_b=5e-7,\n",
    "                  eta=0.7, guidance_scale=7.5, inference_steps=50):\n",
    "        # --- Garder cette méthode telle quelle, elle appelle les autres ---\n",
    "        # ... (elle utilise déjà les méthodes corrigées A, B, C)\n",
    "        if isinstance(input_image, str): input_image_pil = Image.open(input_image).convert(\"RGB\")\n",
    "        elif isinstance(input_image, Image.Image): input_image_pil = input_image\n",
    "        else: # Si c'est un tenseur, essayer de le décoder pour la visualisation\n",
    "             try:\n",
    "                 with torch.no_grad():\n",
    "                     latents = self.vae.encode(input_image).latent_dist.mean # Utiliser la moyenne pour une reconstruction stable\n",
    "                     latents = 1 / self.vae.config.scaling_factor * latents\n",
    "                     rec_image = self.vae.decode(latents.to(self.vae.dtype)).sample\n",
    "                     rec_image = (rec_image / 2 + 0.5).clamp(0, 1).cpu().permute(0, 2, 3, 1).float().numpy()[0]\n",
    "                     input_image_pil = Image.fromarray((rec_image * 255).round().astype(\"uint8\"))\n",
    "             except Exception as e:\n",
    "                 if self.verbose: print(f\"Impossible de décoder le tenseur d'entrée pour la visualisation : {e}\")\n",
    "                 input_image_pil = Image.new('RGB', (512, 512), color = 'grey') # Placeholder\n",
    "\n",
    "        optimized_embedding, latent_input, target_embedding = self.step_a_optimize_text_embedding(\n",
    "            input_image_pil, target_text, num_steps=steps_a, lr=lr_a # Passer PIL pour être sûr\n",
    "        )\n",
    "        _ = self.step_b_finetune_unet( # Pas besoin de stocker le retour, UNet est modifié en place\n",
    "            latent_input, optimized_embedding, num_steps=steps_b, lr=lr_b\n",
    "        )\n",
    "        generated_image, attention_maps = self.step_c_generate_with_attention_control(\n",
    "            target_embedding, optimized_embedding, eta=eta, guidance_scale=guidance_scale, num_inference_steps=inference_steps\n",
    "        )\n",
    "        fig = self.visualize_attention_maps(input_image_pil, generated_image, attention_maps)\n",
    "        os.makedirs(\"./output\", exist_ok=True); generated_image.save(\"./output/edited_image.png\")\n",
    "        if fig: fig.savefig(\"./output/attention_visualization.png\")\n",
    "        return {\"original_image\": input_image_pil, \"edited_image\": generated_image, \"attention_maps\": attention_maps,\n",
    "                \"optimized_embedding\": optimized_embedding, \"target_embedding\": target_embedding, \"figure\": fig}\n",
    "\n",
    "    def register_controlled_attention(self, target_text): # Passer le texte cible\n",
    "        attn_procs = {}\n",
    "        # Encoder les tokens une fois pour le processeur\n",
    "        tokens_pt = self.tokenizer(target_text, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "        target_tokens = tokens_pt.input_ids\n",
    "\n",
    "        for name, module in self.unet.named_modules():\n",
    "            if \"attn2\" in name and isinstance(module, Attention):\n",
    "                 # Donner le tokenizer et les tokens au processeur\n",
    "                 processor_instance = ControlledAttentionProcessor(self.tokenizer, target_tokens)\n",
    "                 module.processor = processor_instance\n",
    "        if self.verbose: print(\"Processeurs d'attention contrôlée enregistrés.\")\n",
    "\n",
    "\n",
    "    # --- DANS step_c_generate_with_attention_control ---\n",
    "        # ... (début inchangé jusqu'à la préparation des embeddings)\n",
    "\n",
    "        # Activer le contrôle d'attention (passer le texte cible)\n",
    "        self.register_controlled_attention(target_text) # Assurez-vous que target_text est disponible ici\n",
    "\n",
    "        # ... (préparation uncond_embeddings)\n",
    "\n",
    "        # NE PAS interpoler ici si le contrôle d'attention gère tout\n",
    "        # text_embeddings = torch.cat([uncond_embeddings, target_embedding]) # Utiliser e_tgt directement ? Ou e_bar ? À tester.\n",
    "        # Pour CFG, il faut quand même l'embedding cible\n",
    "        e_target_for_cfg = interpolated_embedding # Ou juste target_embedding ?\n",
    "        text_embeddings_cfg = torch.cat([uncond_embeddings, e_target_for_cfg])\n",
    "\n",
    "        # ... (initialisation latents_inf)\n",
    "\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps, desc=\"Génération Contrôlée\")):\n",
    "            latent_model_input = torch.cat([latents_inf] * 2)\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "            # Préparer les kwargs pour le processeur d'attention\n",
    "            # Le processeur utilisera e_opt_optimized (source) et l'embedding cible fourni ici\n",
    "            cross_attention_kwargs = {\"e_source\": e_opt_optimized, \"e_target\": e_target_for_cfg}\n",
    "\n",
    "            # Prédire le bruit - le processeur attaché fera la magie\n",
    "            with torch.no_grad():\n",
    "                 noise_pred = self.unet(\n",
    "                     latent_model_input,\n",
    "                     t,\n",
    "                     encoder_hidden_states=text_embeddings_cfg, # L'embedding pour CFG\n",
    "                     cross_attention_kwargs=cross_attention_kwargs # Passer les embeddings source/cible au processeur\n",
    "                 ).sample\n",
    "\n",
    "            # ... (reste de CFG et step) ...\n",
    "\n",
    "        # --- DÉSENGAGER les processeurs après la génération ---\n",
    "        self.unet.set_attn_processor(None) # Ou réassigner les processeurs par défaut\n",
    "\n",
    "        # ... (décodage image) ...\n",
    "        return final_image, {} # Plus besoin de retourner les cartes d'attention ici, elles sont utilisées en interne\n",
    "# --- Classe ImageSequenceGenerator (Garder telle quelle, mais corriger l'appel super) ---\n",
    "class ImageSequenceGenerator(ImageEditingWithImagic):\n",
    "    # Correction: Passer le model_id à la classe parente\n",
    "    def __init__(self, model_id_or_path=\"stabilityai/stable-diffusion-v1-5\", verbose=True, use_auth_token=None):\n",
    "        super().__init__(model_id_or_path=model_id_or_path, verbose=verbose, use_auth_token=use_auth_token)\n",
    "\n",
    "    def generate_sequence(self, input_image, target_text, num_frames=8,\n",
    "                          start_eta=0.1, end_eta=0.9,\n",
    "                          steps_a=100, lr_a=1e-3, # Passer les kwargs aux étapes internes\n",
    "                          steps_b=200, lr_b=5e-7,\n",
    "                          guidance_scale=7.5, inference_steps=50):\n",
    "        if self.verbose: print(f\"\\n--- Génération d'une séquence de {num_frames} images ---\")\n",
    "        # S'assurer que l'image est PIL pour step_a\n",
    "        if isinstance(input_image, str): input_image_pil = Image.open(input_image).convert(\"RGB\")\n",
    "        elif isinstance(input_image, Image.Image): input_image_pil = input_image\n",
    "        else: # Tenseur ? Essayer de décoder ou lever une erreur\n",
    "             raise TypeError(\"generate_sequence attend un chemin d'image ou un objet PIL.Image\")\n",
    "\n",
    "        # Étapes A et B (exécutées une fois)\n",
    "        optimized_embedding, latent_input, target_embedding = self.step_a_optimize_text_embedding(\n",
    "            input_image_pil, target_text, num_steps=steps_a, lr=lr_a\n",
    "        )\n",
    "        _ = self.step_b_finetune_unet(\n",
    "            latent_input, optimized_embedding, num_steps=steps_b, lr=lr_b\n",
    "        )\n",
    "        # Générer les frames\n",
    "        eta_values = np.linspace(start_eta, end_eta, num_frames)\n",
    "        sequence = []\n",
    "        for i, eta in enumerate(tqdm(eta_values, desc=\"Génération de séquence\")):\n",
    "            if self.verbose: print(f\"Frame {i+1}/{num_frames}, eta={eta:.2f}\")\n",
    "            # Passer les bons kwargs à step_c\n",
    "            image, _ = self.step_c_generate_with_attention_control(\n",
    "                target_embedding, optimized_embedding, eta=eta,\n",
    "                guidance_scale=guidance_scale, num_inference_steps=inference_steps\n",
    "            )\n",
    "            sequence.append(image)\n",
    "        # Sauvegarder et visualiser\n",
    "        os.makedirs(\"./output\", exist_ok=True)\n",
    "        if sequence:\n",
    "             try:\n",
    "                  # Utiliser imageio pour une meilleure compatibilité GIF\n",
    "                  import imageio\n",
    "                  imageio.mimsave('./output/sequence.gif', [np.array(img) for img in sequence], duration=0.2, loop=0) # loop=0 pour boucle infinie\n",
    "                  print(\"GIF sauvegardé dans ./output/sequence.gif\")\n",
    "             except ImportError:\n",
    "                  print(\"Le paquet 'imageio' n'est pas installé. Sauvegarde GIF annulée. Installez avec 'pip install imageio'\")\n",
    "             except Exception as e:\n",
    "                  print(f\"Erreur lors de la sauvegarde du GIF : {e}\")\n",
    "        # Visualisation\n",
    "        if sequence:\n",
    "             cols = min(len(sequence), 8) # Afficher max 8 frames en ligne\n",
    "             rows = (len(sequence) - 1) // cols + 1\n",
    "             fig, axs = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "             axs = axs.flatten()\n",
    "             for i, img in enumerate(sequence):\n",
    "                 axs[i].imshow(img); axs[i].set_title(f\"eta={eta_values[i]:.2f}\"); axs[i].axis(\"off\")\n",
    "             for j in range(i + 1, len(axs)): axs[j].axis(\"off\") # Cacher les axes vides\n",
    "             plt.tight_layout(); plt.show()\n",
    "             if fig: fig.savefig(\"./output/sequence_visualization.png\")\n",
    "        return sequence\n",
    "\n",
    "\n",
    "# --- Exemple d'utilisation Corrigé ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- MODIFIER CE CHEMIN avec votre image ---\n",
    "    INPUT_IMAGE_PATH = \"/home/antoine/genai_project/assets/dog.jpg\"\n",
    "    if not os.path.exists(INPUT_IMAGE_PATH):\n",
    "         print(f\"ERREUR: Le fichier image '{INPUT_IMAGE_PATH}' n'existe pas. Veuillez modifier le chemin.\")\n",
    "         # Mettre une image placeholder pour éviter de planter si on veut tester le code\n",
    "         INPUT_IMAGE_PATH = Image.new('RGB', (512, 512), color = 'gray') # Ou raise FileNotFoundError\n",
    "         print(\"Utilisation d'une image grise placeholder.\")\n",
    "\n",
    "    # --- MODIFIER CE PROMPT pour correspondre à votre image et édition ---\n",
    "    TARGET_TEXT = \"A dog with a blue hat\"\n",
    "\n",
    "    # --- CHOISIR LE MODÈLE (ID Hugging Face ou chemin local) ---\n",
    "    # Utiliser SD 1.5 est recommandé pour commencer avec Imagic/Prompt-to-Prompt\n",
    "    MODEL_ID_OR_PATH = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "    # Si vous avez téléchargé localement :\n",
    "    # MODEL_ID_OR_PATH = \"/chemin/absolu/vers/votre/dossier/stable-diffusion-v1-5\"\n",
    "\n",
    "    print(\"\\n--- Initialisation pour Édition Simple ---\")\n",
    "    try:\n",
    "        # Créer l'instance en utilisant l'ID/chemin choisi\n",
    "        imagic_editor = ImageEditingWithImagic(model_id_or_path=MODEL_ID_OR_PATH, verbose=True)\n",
    "\n",
    "        print(\"\\n--- Démarrage de l'édition ---\")\n",
    "        results = imagic_editor.edit_image(\n",
    "            input_image=INPUT_IMAGE_PATH, # Passer le chemin ou l'objet PIL\n",
    "            target_text=TARGET_TEXT,\n",
    "            steps_a=100,    # Nombre d'étapes pour optimiser l'embedding\n",
    "            lr_a=1e-3,      # Taux d'apprentissage pour l'embedding\n",
    "            steps_b=150,    # Nombre d'étapes pour fine-tuner UNet (ajuster selon ressources)\n",
    "            lr_b=5e-7,      # Taux d'apprentissage pour UNet (très petit)\n",
    "            eta=0.7,        # Poids de l'embedding cible dans l'interpolation\n",
    "            guidance_scale=7.5, # Force de la guidance CFG\n",
    "            inference_steps=50  # Nombre d'étapes pour générer l'image finale\n",
    "        )\n",
    "        print(\"\\n--- Édition simple terminée ---\")\n",
    "        # results[\"edited_image\"].show() # Afficher l'image finale\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! ERREUR lors de l'édition simple: {e} !!!\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Afficher la trace complète pour le débogage\n",
    "\n",
    "\n",
    "    # --- Test de la génération de séquence (optionnel) ---\n",
    "    # print(\"\\n--- Initialisation pour Génération de Séquence ---\")\n",
    "    # try:\n",
    "    #     # Vous pouvez utiliser le même modèle ou un autre\n",
    "    #     seq_gen = ImageSequenceGenerator(model_id_or_path=MODEL_ID_OR_PATH, verbose=True)\n",
    "    #\n",
    "    #     print(\"\\n--- Démarrage de la génération de séquence ---\")\n",
    "    #     sequence_images = seq_gen.generate_sequence(\n",
    "    #         input_image=INPUT_IMAGE_PATH,\n",
    "    #         target_text=TARGET_TEXT,\n",
    "    #         num_frames=5,     # Nombre d'images dans le GIF/séquence\n",
    "    #         start_eta=0.3,    # eta de départ pour l'interpolation\n",
    "    #         end_eta=0.9,      # eta de fin\n",
    "    #         steps_a=50,       # Peut-être réduire pour accélérer la séquence\n",
    "    #         steps_b=100,      # Peut-être réduire pour accélérer la séquence\n",
    "    #         inference_steps=30 # Peut-être réduire pour accélérer la séquence\n",
    "    #     )\n",
    "    #     print(\"\\n--- Génération de séquence terminée ---\")\n",
    "    #\n",
    "    # except Exception as e:\n",
    "    #     print(f\"\\n!!! ERREUR lors de la génération de séquence: {e} !!!\")\n",
    "    #     import traceback\n",
    "    #     traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
